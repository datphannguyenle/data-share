""
""
"GLOBAL "
"EDITION"
"This is a special edition of an established "
"title widely used by colleges and universities "
"throughout the world. Pearson published this "
"exclusive edition for the benefit of students "
"outside the United States and Canada. If "
"you purchased this book within the United "
"States or Canada, you should be aware that "
"it has been imported without the approval of "
"the Publisher or Author. The Global Edition "
"is not supported in the United States and "
"Canada."
"Pearson Global Edition"
"GLOBAL "
"EDITION"
"For these Global Editions, the editorial team at Pearson has "
"collaborated with educators across the world to address a "
"wide range of subjects and requirements, equipping students "
"with the best possible learning tools. This Global Edition "
"preserves the cutting-edge approach and pedagogy of the "
"original, but also features alterations, customization, and "
"adaptation from the North American version."
"Digital Image Processing"
" FOURTH  EDITION"
" Rafael C. Gonzalez • Richard E. Woods"
"Digital Image Processing"
"Gonzalez Woods"
"FOURTH"
" "
"EDITION"
"GLOBAL"
" "
"EDITION"
"Gonzalez_04_1292223049_Final.indd   111/08/17   5:27 PM"
"www.EBooks","World.ir"
""
"Support Package for Digital "
"Image Processing"
"Your new textbook provides access to support packages that may include reviews in areas "
"like probability and vectors, tutorials on topics relevant to the material in the book, an image "
"database, and more. Refer to the Preface in the textbook for a detailed list of resources."
"Follow the instructions below to register for the Companion Website for Rafael C. Gonzalez and "
"Richard E. Woods’ Digital Image Processing, Fourth Edition, Global Edition."
"1.  Go to www.Image","Processing","Place.com"
"2.  Find the title of your textbook."
"3.    Click Support Materials and follow the on-screen instructions to create a login name and "
"password."
"Use the login name and password you created during registration to start using the "
"digital resources that accompany your textbook."
"IMPORTANT:"
"This serial code can only be used once. This subscription is not transferrable."
"Gonzalez_04_1292223049_ifc_Final.indd   111/08/17   5:33 PM"
"www.EBooks","World.ir"
""
"Processing"
"igital Image"
"4"
"D"
"FOURTH"
"EDITION"
"Rafael C. Gonzalez"
"University of Tennessee"
"Richard E. Woods"
"Interapptics"
"330 Hudson Street, New York, NY 10013"
"Global Edition"
"DIP","4E_GLOBAL_Print_Ready.indb   17/6/2017   10:55:08 AM"
"www.EBooks","World.ir"
""
"Senior Vice President Courseware Portfolio Management: Marcia J. Horton"
"Director, Portfolio Management: Engineering, Computer Science & Global Editions: Julian Partridge"
"Portfolio Manager: Julie Bai "
"Field Marketing Manager: Demetrius Hall "
"Product Marketing Manager: Yvonne Vannatta "
"Marketing Assistant: Jon Bryant "
"Content Managing Producer, ECS and Math: Scott Disanno "
"Content Producer: Michelle Bayman "
"Project Manager: Rose Kernan"
"Assistant Project Editor, Global Editions: Vikash Tiwari"
"Operations Specialist: Maura Zaldivar-Garcia "
"Manager, Rights and Permissions: Ben Ferrini "
"Senior Manufacturing Controller, Global Editions: Trudy Kimber "
"Media Production Manager, Global Editions: Vikram Kumar"
"Cover Designer: Lumina Datamatics "
"Cover  Photo: CT  image—© zhuravliki.123rf.com/Pearson  Asset  Library; Gram-negative  bacteria—© royaltystockphoto.com/"
"Shutterstock.com; Orion  Nebula—© creativemarc/Shutterstock.com; Fingerprints—© Larysa   Ray/Shutterstock.com; Cancer "
"cells—© Greenshoots Communications/Alamy Stock Photo"
"MATLAB is a registered trademark of The Math","Works, Inc., 1 Apple Hill Drive, Natick, MA 01760-2098."
"Pearson Education Limited"
"Edinburgh Gate"
"Harlow"
"Essex CM","20 2JE"
"England"
"and Associated Companies throughout the world"
"Visit us on the World Wide Web at: "
"www.pearsonglobaleditions.com"
"© Pearson Education Limited 2018"
"The rights of Rafael C. Gonzalez and Richard E. Woods to be identified as the authors of this work have been asserted by them "
"in accordance with the Copyright, Designs and Patents Act 1988."
"Authorized adaptation from the United States edition, entitled Digital Image Processing, Fourth Edition, ISBN 978-0-13-335672-4, "
"by Rafael C. Gonzalez and Richard E. Woods, published by Pearson Education © 2018."
"All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by "
"any means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the pub-"
"lisher or a license permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, Saffron "
"House, 6–10 Kirby Street, London EC","1N 8TS."
"All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the "
"author or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliation "
"with or endorsement of this book by such owners."
"British Library Cataloguing-in-Publication Data"
"A catalogue record for this book is available from the British Library"
"10 9 8 7 6 5 4 3 2 1"
"ISBN 10: 1-292-22304-9"
"ISBN 13: 978-1-292-22304-9"
"Typeset by Richard E. Woods"
"Printed and bound in Malaysia"
"DIP","4E_GLOBAL_Print_Ready.indb   27/6/2017   10:55:08 AM"
"www.EBooks","World.ir"
""
"To Connie, Ralph, and Rob "
"and"
"To Janice, David, and Jonathan"
"DIP","4E_GLOBAL_Print_Ready.indb   36/16/2017   2:01:57 PM"
"www.EBooks","World.ir"
""
"DIP","4E_GLOBAL_Print_Ready.indb   46/16/2017   2:01:57 PM"
"This page intentionally left blank"
"www.EBooks","World.ir"
""
"Contents"
"Preface        9"
"Acknowledgments        12"
"The Book Website      13"
"The DIP","4E Support Packages      13"
"About the Authors      14"
"1"
"   Introduction       17"
"What is Digital Image Processing?      18"
"The Origins of Digital Image Processing      19"
"Examples of Fields that Use Digital Image Processing      23"
"Fundamental Steps in Digital Image Processing      41"
"Components of an Image Processing System      44"
"2"
"   Digital Image Fundamentals      47"
"Elements of Visual Perception      48"
"Light and the Electromagnetic Spectrum      54"
"Image Sensing and Acquisition      57"
"Image Sampling and Quantization      63"
"Some Basic Relationships Between Pixels      79"
"Introduction to the Basic Mathematical Tools Used in Digital Image "
"Processing        83"
"3"
"   Intensity Transformations and Spatial  "
"Filtering        119"
"Background        120"
"Some Basic Intensity Transformation Functions      122"
"Histogram Processing      133"
"Fundamentals of Spatial Filtering      153"
"Smoothing (Lowpass) Spatial Filters      164"
"Sharpening (Highpass) Spatial Filters      175"
"Highpass, Bandreject, and Bandpass Filters from Lowpass Filters      188"
"Combining Spatial Enhancement Methods      191"
"DIP","4E_GLOBAL_Print_Ready.indb   56/16/2017   2:01:57 PM"
"www.EBooks","World.ir"
""
"6"
"    "
"Contents"
"4"
"   Filtering in the Frequency "
"Domain        203"
"Background        204"
"Preliminary Concepts      207"
"Sampling and the Fourier Transform of Sampled  "
"Functions        215"
"The Discrete Fourier Transform of One Variable      225"
"Extensions to Functions of Two Variables      230"
"Some Properties of the 2-D DFT and IDFT      240"
"The Basics of Filtering in the Frequency Domain      260"
"Image Smoothing Using Lowpass Frequency Domain  "
"Filters        272"
"Image Sharpening Using Highpass Filters      284"
"Selective Filtering      296"
"The Fast Fourier Transform      303"
"5"
"   Image   Restoration   "
"and Reconstruction      317"
"A Model of the Image Degradation/Restoration  "
"process        318"
"Noise Models      318"
"Restoration in the Presence of Noise Only—Spatial Filtering      327"
"Periodic Noise Reduction Using Frequency Domain Filtering      340"
"Linear, Position-Invariant Degradations      348"
"Estimating the Degradation Function      352"
"Inverse Filtering      356"
"Minimum Mean Square Error (Wiener) Filtering      358"
"Constrained Least Squares Filtering      363"
"Geometric Mean Filter      367"
"Image Reconstruction from Projections      368"
"6   Color Image Processing      399"
"Color Fundamentals      400"
"Color Models      405"
"Pseudocolor Image Processing      420"
"Basics of Full-Color Image Processing      429"
"Color Transformations      430"
"DIP","4E_GLOBAL_Print_Ready.indb   66/16/2017   2:01:57 PM"
"www.EBooks","World.ir"
""
"Contents"
"    "
"7"
"Color Image Smoothing and Sharpening      442"
"Using Color in Image Segmentation      445"
"Noise in Color Images      452"
"Color Image Compression      455"
"7   Wavelet and Other Image Transforms      463"
"Preliminaries        464"
"Matrix-based Transforms      466"
"Correlation        478"
"Basis Functions in the Time-Frequency Plane      479"
"Basis Images      483"
"Fourier-Related Transforms      484"
"Walsh-Hadamard Transforms      496"
"Slant Transform      500"
"Haar Transform      502"
"Wavelet Transforms     504"
"8"
"   Image Compression and  "
"Watermarking        539"
"Fundamentals        540"
"Huffman Coding      553"
"Golomb Coding      556"
"Arithmetic Coding      561"
"LZW Coding      564"
"Run-length Coding      566"
"Symbol-based Coding      572"
"Bit-plane Coding      575"
"Block Transform Coding      576"
"Predictive Coding     594"
"Wavelet Coding     614"
"Digital Image Watermarking     624"
"9   Morphological Image Processing      635"
"Preliminaries        636"
"Erosion and Dilation      638"
"Opening and Closing      644"
"The Hit-or-Miss Transform      648"
"DIP","4E_GLOBAL_Print_Ready.indb   76/16/2017   2:01:57 PM"
"www.EBooks","World.ir"
""
"8"
"    "
"Contents"
"Some Basic Morphological Algorithms      652"
"Morphological Reconstruction      667"
"Summary of Morphological Operations on Binary Images      673"
"Grayscale Morphology      674"
"10 Image Segmentation      699"
"Fundamentals        700"
"Point, Line, and Edge Detection     701"
"Thresholding        742"
"Segmentation by Region Growing and by Region Splitting and "
"Merging        764"
"Region Segmentation Using Clustering and  "
"Superpixels        770"
"Region Segmentation Using Graph Cuts  777"
"Segmentation Using Morphological Watersheds  786"
"The Use of Motion in Segmentation     796"
"11"
" Feature Extraction       811"
"Background        812"
"Boundary Preprocessing     814"
"Boundary Feature Descriptors     831"
"Region Feature Descriptors     840"
"Principal Components as Feature Descriptors     859"
"Whole-Image Features     868"
"Scale-Invariant Feature Transform (SIFT)     881"
"12"
" Image Pattern Classification      903"
"Background        904"
"Patterns and Pattern Classes     906"
"Pattern Classification by Prototype Matching     910"
"Optimum (Bayes) Statistical Classifiers     923"
"Neural Networks and Deep Learning     931"
"Deep Convolutional Neural Networks     964"
"Some Additional Details of Implementation     987"
"Bibliography        995"
"Index        1009"
"DIP","4E_GLOBAL_Print_Ready.indb   87/12/2017   10:23:39 AM"
"www.EBooks","World.ir"
""
"47"
"2"
"Digital Image Fundamentals"
"Preview"
"This chapter is an introduction to a number of basic concepts in digital image processing that are used "
"throughout the book. Section 2.1 summarizes some important aspects of the human visual system, includ-"
"ing image formation in the eye and its capabilities for brightness adaptation and discrimination. Section "
"2.2 discusses light, other components of the electromagnetic spectrum, and their imaging characteristics. "
"Section 2.3 discusses imaging sensors and how they are used to generate digital images. Section 2.4 intro-"
"duces the concepts of uniform image sampling and intensity quantization. Additional topics discussed "
"in that section include digital image representation, the effects of varying the number of samples and "
"intensity levels in an image, the concepts of spatial and intensity resolution, and the principles of image "
"interpolation. Section 2.5 deals with a variety of basic relationships between pixels. Finally, Section 2.6 "
"is an introduction to the principal mathematical tools we use throughout the book. A second objective "
"of that section is to help you begin developing a “feel” for how these tools are used in a variety of basic "
"image processing tasks. "
"Upon completion of this chapter, readers should:"
"  Have  an  understanding  of  some  important  "
"functions and limitations of human vision."
"   Be  familiar  with  the  electromagnetic  energy  "
"spectrum, including basic properties of light."
"   Know  how  digital  images  are  generated  and  "
"represented."
"  Understand the basics of image sampling and "
"quantization."
"   Be  familiar  with  spatial  and  intensity  resolu-"
"tion and their effects on image appearance."
"  Have  an  understanding  of  basic  geometric  "
"relationships between image pixels."
"   Be  familiar  with  the  principal  mathematical  "
"tools used in digital image processing."
"  Be able to apply a variety of introductory dig-"
"ital image processing techniques."
"Those who wish to succeed must ask the right preliminary "
"questions."
"Aristotle"
"DIP","4E_GLOBAL_Print_Ready.indb   476/16/2017   2:02:02 PM"
"www.EBooks","World.ir"
""
"48"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"2.1 ELEMENTS OF VISUAL PERCEPTION  "
"Although the field of digital image processing is built on a foundation of mathemat-"
"ics,  human  intuition  and  analysis  often  play  a  role  in  the  choice  of  one  technique  "
"versus another, and this choice often is made based on subjective, visual judgments. "
"Thus, developing an understanding of basic characteristics of human visual percep-"
"tion as a first step in our journey through this book is appropriate. In particular, our "
"interest  is  in  the  elementary  mechanics  of  how  images  are  formed  and  perceived  "
"by  humans.  We  are  interested  in  learning  the  physical  limitations  of  human  vision  "
"in terms of factors that also are used in our work with digital images. Factors such "
"as how human and electronic imaging devices compare in terms of resolution and "
"ability  to  adapt  to  changes  in  illumination  are  not  only  interesting,  they  are  also  "
"important from a practical point of view."
"STRUCTURE OF THE HUMAN EYE"
"Figure  2.1  shows  a  simplified  cross  section  of  the  human  eye.  The  eye  is  nearly  a  "
"sphere (with a diameter of about 20 mm) enclosed by three membranes: the cornea "
"and sclera outer cover; the choroid; and the retina. The cornea is a tough, transparent "
"tissue that covers the anterior surface of the eye. Continuous with the cornea, the "
"sclera is an opaque membrane that encloses the remainder of the optic globe."
"The choroid lies directly below the sclera. This membrane contains a network of "
"blood vessels that serve as the major source of nutrition to the eye. Even superficial "
"2.1"
"Retina"
"Blind spot"
"Sclera"
"Choroid"
"Nerve & sheath"
"Fovea"
"Vitreous humor"
"Visual axis"
"Ciliary fibers"
"Ciliary muscle"
"Iris"
"Cornea"
"Lens"
"Anterior  chamber"
"Ciliary body"
"FIGURE 2.1"
"Simplified  "
"diagram of a  "
"cross section of "
"the human eye."
"DIP","4E_GLOBAL_Print_Ready.indb   486/16/2017   2:02:02 PM"
"www.EBooks","World.ir"
""
"2.1"
"  "
"Elements of Visual Perception"
"    "
"49"
"injury to the choroid can lead to severe eye damage as a result of inflammation that "
"restricts blood flow. The choroid coat is heavily pigmented, which helps reduce the "
"amount  of  extraneous  light  entering  the  eye  and  the  backscatter  within  the  optic  "
"globe. At its anterior extreme, the choroid is divided into the ciliary body and the "
"iris. The latter contracts or expands to control the amount of light that enters the eye. "
"The central opening of the iris (the pupil) varies in diameter from approximately 2 "
"to 8 mm. The front of the iris contains the visible pigment of the eye, whereas the "
"back contains a black pigment."
"The lens consists of concentric layers of fibrous cells and is suspended by fibers "
"that attach to the ciliary body. It is composed of 60% to 70% water, about 6% fat, "
"and more protein than any other tissue in the eye. The lens is colored by a slightly "
"yellow pigmentation that increases with age. In extreme cases, excessive clouding of "
"the  lens,  referred  to  as  cataracts,  can  lead  to  poor  color  discrimination  and  loss  of  "
"clear vision. The lens absorbs approximately 8% of the visible light spectrum, with "
"higher  absorption  at  shorter  wavelengths.  Both  infrared  and  ultraviolet  light  are  "
"absorbed by proteins within the lens and, in excessive amounts, can damage the eye."
"The  innermost  membrane  of  the  eye  is  the  retina,  which  lines  the  inside  of  the  "
"wall’s  entire  posterior  portion.  When  the  eye  is  focused,  light  from  an  object  is  "
"imaged  on  the  retina.  Pattern  vision  is  afforded  by  discrete  light  receptors  distrib-"
"uted over the surface of the retina. There are two types of receptors: cones and rods. "
"There are between 6 and 7 million cones in each eye. They are located primarily in "
"the central portion of the retina, called the fovea, and are highly sensitive to color. "
"Humans can resolve fine details because each cone is connected to its own nerve end. "
"Muscles rotate the eye until the image of a region of interest falls on the fovea. Cone "
"vision is called photopic or bright-light vision."
"The number of rods is much larger: Some 75 to 150 million are distributed over "
"the retina. The larger area of distribution, and the fact that several rods are connect-"
"ed to a single nerve ending, reduces the amount of detail discernible by these recep-"
"tors.  Rods  capture  an  overall  image  of  the  field  of  view.  They  are  not  involved  in  "
"color vision, and are sensitive to low levels of illumination. For example, objects that "
"appear brightly colored in daylight appear as colorless forms in moonlight because "
"only  the  rods  are  stimulated.  This  phenomenon  is  known  as  scotopic  or  dim-light "
"vision."
"Figure 2.2 shows the density of rods and cones for a cross section of the right eye, "
"passing through the region where the optic nerve emerges from the eye. The absence "
"of receptors in this area causes the so-called blind spot (see Fig. 2.1). Except for this "
"region, the distribution of receptors is radially symmetric about the fovea. Receptor "
"density is measured in degrees from the visual axis. Note in Fig. 2.2 that cones are "
"most dense in the center area of the fovea, and that rods increase in density from "
"the center out to approximately 20° off axis. Then, their density decreases out to the "
"periphery of the retina."
"The fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, "
"so it has an area of approximately 1.77 "
"mm"
"2"
". As Fig. 2.2 shows, the density of cones "
"in  that  area  of  the  retina  is  on  the  order  of  150,000  elements  per  mm"
"2"
".  Based  on  "
"these figures, the number of cones in the fovea, which is the region of highest acuity "
"DIP","4E_GLOBAL_Print_Ready.indb   496/16/2017   2:02:03 PM"
"www.EBooks","World.ir"
""
"50"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"in the eye, is about 265,000 elements. Modern electronic imaging chips exceed this "
"number by a large factor. While the ability of humans to integrate intelligence and "
"experience with vision makes purely quantitative comparisons somewhat superficial, "
"keep in mind for future discussions that electronic imaging sensors can easily exceed "
"the capability of the eye in resolving image detail."
"IMAGE FORMATION IN THE EYE"
"In an ordinary photographic camera, the lens has a fixed focal length. Focusing at "
"various distances is achieved by varying the distance between the lens and the imag-"
"ing plane, where the film (or imaging chip in the case of a digital camera) is located. "
"In the human eye, the converse is true; the distance between the center of the lens "
"and the imaging sensor (the retina) is fixed, and the focal length needed to achieve "
"proper  focus  is  obtained  by  varying  the  shape  of  the  lens.  The  fibers  in  the  ciliary  "
"body  accomplish  this  by  flattening  or  thickening  the  lens  for  distant  or  near  ob-"
"jects, respectively. The distance between the center of the lens and the retina along "
"the visual axis is approximately 17 mm. The range of focal lengths is approximately "
"14 mm to 17 mm, the latter taking place when the eye is relaxed and focused at dis-"
"tances greater than about 3 m. The geometry in Fig. 2.3 illustrates how to obtain the "
"dimensions of an image formed on the retina. For example, suppose that a person "
"is  looking  at  a  tree  15  m  high  at  a  distance  of  100  m.  Letting  h  denote  the  height  "
"of that object in the retinal image, the geometry of Fig. 2.3 yields "
"15 10017=h or "
"h=25."
" mm. As indicated earlier in this section, the retinal image is focused primar-"
"ily on the region of the fovea. Perception then takes place by the relative excitation "
"of light receptors, which transform radiant energy into electrical impulses that ulti-"
"mately are decoded by the brain."
"BRIGHTNESS ADAPTATION AND DISCRIMINATION"
"Because  digital  images  are  displayed  as  sets  of  discrete  intensities,  the  eye’s  abil-"
"ity to discriminate between different intensity levels is an important consideration "
"FIGURE 2.2"
"Distribution of "
"rods and cones in "
"the retina."
"Blind spot"
"Cones"
"Rods"
"No. of rods or cones per mm"
"2"
"Degrees from visual axis (center of fovea)"
"180,000"
"135,000"
"90,000"
"45,000"
"80604020020406080"
"DIP","4E_GLOBAL_Print_Ready.indb   506/16/2017   2:02:03 PM"
"www.EBooks","World.ir"
""
"2.1"
"  "
"Elements of Visual Perception"
"51"
"in presenting image processing results. The range of light intensity levels to which "
"the human visual system can adapt is enormous—on the order of 10"
"10"
"— from the "
"scotopic  threshold  to  the  glare  limit.  Experimental  evidence  indicates  that  subjec-"
"tive brightness (intensity as perceived by the human visual system) is a logarithmic "
"function  of  the  light  intensity  incident  on  the  eye.  Figure  2.4,  a  plot  of  light  inten-"
"sity versus subjective brightness, illustrates this characteristic. The long solid curve "
"represents the range of intensities to which the visual system can adapt. In photopic "
"vision alone, the range is about 10"
"6"
". The transition from scotopic to photopic vision "
"is gradual over the approximate range from 0.001 to 0.1 millilambert (−3 to −1 m","L "
"in the log scale), as the double branches of the adaptation curve in this range show."
"The key point in interpreting the impressive dynamic range depicted in Fig. 2.4 "
"is that the visual system cannot operate over such a range simultaneously. Rather, it "
"accomplishes this large variation by changing its overall sensitivity, a phenomenon "
"known as brightness adaptation. The total range of distinct intensity levels the eye "
"can discriminate simultaneously is rather small when compared with the total adap-"
"tation range. For a given set of conditions, the current sensitivity level of the visual "
"system is called the brightness adaptation level, which may correspond, for example, "
"FIGURE 2.3"
"Graphical  "
"representation of "
"the eye looking at "
"a palm tree. Point "
"C is the focal  "
"center of the lens."
"15 m"
"C"
"17 mm","100 m"
"FIGURE 2.4"
"Range of subjec-"
"tive brightness "
"sensations  "
"showing a  "
"particular  "
"adaptation level, "
"B"
"a"
"."
"Glare limit"
"Subjective brightness"
"Adaptation range"
"Scotopic"
"threshold"
"Log of intensity (m","L)"
"Scotopic"
"Photopic"
"6420 2 4"
"B"
"a"
"B"
"b"
"DIP","4E_GLOBAL_Print_Ready.indb   516/16/2017   2:02:03 PM"
"www.EBooks","World.ir"
""
"52"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"to brightness B"
"a"
" in Fig. 2.4. The short intersecting curve represents the range of sub-"
"jective brightness that the eye can perceive when adapted to this level. This range is "
"rather restricted, having a level B"
"b"
" at, and below which, all stimuli are perceived as "
"indistinguishable blacks. The upper portion of the curve is not actually restricted but, "
"if extended too far, loses its meaning because much higher intensities would simply "
"raise the adaptation level higher than B"
"a"
"."
"The  ability  of  the  eye  to  discriminate  between  changes  in  light  intensity  at  any  "
"specific  adaptation  level  is  of  considerable  interest.  A  classic  experiment  used  to  "
"determine the capability of the human visual system for brightness discrimination "
"consists of having a subject look at a flat, uniformly illuminated area large enough to "
"occupy the entire field of view. This area typically is a diffuser, such as opaque glass, "
"illuminated from behind by a light source, I, with variable intensity. To this field is "
"added  an  increment  of  illumination,  "
"I"
",  in  the  form  of  a  short-duration  flash  that  "
"appears as a circle in the center of the uniformly illuminated field, as Fig. 2.5 shows."
"If I is not bright enough, the subject says “no,” indicating no perceivable change. "
"As I gets stronger, the subject may give a positive response of “yes,” indicating a "
"perceived change. Finally, when I is strong enough, the subject will give a response "
"of “yes” all the time. The quantity II"
"c"
", where I"
"c"
" is the increment of illumination "
"discriminable 50% of the time with background illumination I, is called the Weber "
"ratio.  A  small  value  of  II"
"c"
"  means  that  a  small  percentage  change  in  intensity  is  "
"discriminable. This represents “good” brightness discrimination. Conversely, a large "
"value of II"
"c"
" means that a large percentage change in intensity is required for the "
"eye to detect the change. This represents “poor” brightness discrimination."
"A plot of II"
"c"
" as a function of log","I has the characteristic shape shown in Fig. 2.6. "
"This curve shows that brightness discrimination is poor (the Weber ratio is large) at "
"low levels of illumination, and it improves significantly (the Weber ratio decreases) "
"as background illumination increases. The two branches in the curve reflect the fact "
"that at low levels of illumination vision is carried out by the rods, whereas, at high "
"levels, vision is a function of cones."
"If  the  background  illumination  is  held  constant  and  the  intensity  of  the  other  "
"source, instead of flashing, is now allowed to vary incrementally from never being "
"perceived to always being perceived, the typical observer can discern a total of one "
"to two dozen different intensity changes. Roughly, this result is related to the num-"
"ber of different intensities a person can see at any one point or small area in a mono-"
"chrome image. This does not mean that an image can be represented by such a small "
"number  of  intensity  values  because,  as  the  eye  roams  about  the  image,  the  average  "
"FIGURE 2.5  "
"Basic"
"experimental  "
"setup used to "
"characterize "
"brightness  "
"discrimination."
"I"
"I "
"I"
"+"
"DIP","4E_GLOBAL_Print_Ready.indb   526/16/2017   2:02:04 PM"
"www.EBooks","World.ir"
""
"2.1"
"  "
"Elements of Visual Perception"
"    "
"53"
"background changes, thus allowing a different set of incremental changes to be detect-"
"ed at each new adaptation level. The net result is that the eye is capable of a broader "
"range of overall intensity discrimination. In fact, as we will show in Section 2.4, the eye "
"is  capable  of  detecting  objectionable  effects  in  monochrome  images  whose  overall  "
"intensity is represented by fewer than approximately two dozen levels."
"Two phenomena demonstrate that perceived brightness is not a simple function "
"of intensity. The first is based on the fact that the visual system tends to undershoot "
"or overshoot around the boundary of regions of different intensities. Figure 2.7(a) "
"shows a striking example of this phenomenon. Although the intensity of the stripes "
"FIGURE 2.6"
"A typical plot of "
"the Weber ratio "
"as a function of "
"intensity."
"1.5"
"2.0"
"43210"
"log I"
"log "
""
"I"
"c"
"/"
"I"
"1234"
"1.0"
"0.5"
"0.5"
"1.0"
"0"
"Actual intensity"
"Perceived intensity"
"FIGURE 2.7"
"Illustration of the "
"Mach band effect. "
"Perceived  "
"intensity is not a "
"simple function of "
"actual intensity."
"b"
"a"
"c"
"DIP","4E_GLOBAL_Print_Ready.indb   536/16/2017   2:02:05 PM"
"www.EBooks","World.ir"
""
"54"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"is constant [see Fig. 2.7(b)], we actually perceive a brightness pattern that is strongly "
"scalloped near the boundaries, as Fig. 2.7(c) shows. These perceived scalloped bands "
"are called Mach bands after Ernst Mach, who first described the phenomenon in 1865."
"The  second  phenomenon,  called  simultaneous  contrast,  is  that  a  region’s  per-"
"ceived brightness does not depend only on its intensity, as Fig. 2.8 demonstrates. All "
"the center squares have exactly the same intensity, but each appears to the eye to "
"become darker as the background gets lighter. A more familiar example is a piece of "
"paper that looks white when lying on a desk, but can appear totally black when used "
"to shield the eyes while looking directly at a bright sky."
"Other examples of human perception phenomena are optical illusions, in which "
"the  eye  fills  in  nonexisting  details  or  wrongly  perceives  geometrical  properties  of  "
"objects.  Figure  2.9  shows  some  examples.  In  Fig.  2.9(a),  the  outline  of  a  square  is  "
"seen clearly, despite the fact that no lines defining such a figure are part of the image. "
"The same effect, this time with a circle, can be seen in Fig. 2.9(b); note how just a few "
"lines are sufficient to give the illusion of a complete circle. The two horizontal line "
"segments in Fig. 2.9(c) are of the same length, but one appears shorter than the other. "
"Finally, all long lines in Fig. 2.9(d) are equidistant and parallel. Yet, the crosshatching "
"creates the illusion that those lines are far from being parallel."
"2.2 LIGHT AND THE ELECTROMAGNETIC SPECTRUM  "
"The electromagnetic spectrum was introduced in Section 1.3. We now consider this "
"topic  in  more  detail.  In  1666,  Sir  Isaac  Newton  discovered  that  when  a  beam  of  "
"sunlight passes through a glass prism, the emerging beam of light is not white but "
"consists instead of a continuous spectrum of colors ranging from violet at one end "
"to red at the other. As Fig. 2.10 shows, the range of colors we perceive in visible light "
"is a small portion of the electromagnetic spectrum. On one end of the spectrum are "
"radio waves with wavelengths billions of times longer than those of visible light. On "
"the other end of the spectrum are gamma rays with wavelengths millions of times "
"smaller than those of visible light. We showed examples in Section 1.3 of images in "
"most of the bands in the EM spectrum."
"2.2"
"bac"
"FIGURE  2.8"
" Examples of simultaneous contrast. All the inner squares have the same intensity, "
"but they appear progressively darker as the background becomes lighter."
"DIP","4E_GLOBAL_Print_Ready.indb   546/16/2017   2:02:05 PM"
"www.EBooks","World.ir"
""
"2.2"
"  "
"Light and the Electromagnetic Spectrum"
"    "
"55"
"The electromagnetic spectrum can be expressed in terms of wavelength, frequency, "
"or energy. Wavelength (l) and frequency (n) are related by the expression "
" l"
"n"
"="
"c"
" "
"(2-1)"
"where c is the speed of light ("
"299810"
"8"
".* m/s). Figure 2.11 shows a schematic repre-"
"sentation of one wavelength. "
"The energy of the various components of the electromagnetic spectrum is given "
"by the expression"
" Eh=n "
"(2-2)"
"where h  is  Planck’s  constant.  The  units  of  wavelength  are  meters,  with  the  terms  "
"microns (denoted mm  and equal to 10"
"6−"
" m) and nanometers (denoted nm and equal "
"to 10"
"9−"
" m) being used just as frequently. Frequency is measured in Hertz (Hz), with "
"one Hz being equal to one cycle of a sinusoidal wave per second. A commonly used "
"unit of energy is the electron-volt."
"Electromagnetic  waves  can  be  visualized  as  propagating  sinusoidal  waves  with  "
"wavelength l (Fig. 2.11), or they can be thought of as a stream of massless particles, "
"ba"
"dc"
"FIGURE 2.9  "
"Some "
"well-known  "
"optical illusions."
"DIP","4E_GLOBAL_Print_Ready.indb   556/16/2017   2:02:06 PM"
"www.EBooks","World.ir"
""
"56"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"each  traveling  in  a  wavelike  pattern  and  moving  at  the  speed  of  light.  Each  mass-"
"less  particle  contains  a  certain  amount  (or  bundle)  of  energy,  called  a  photon.  We  "
"see from Eq. (2-2) that energy is proportional to frequency, so the higher-frequency "
"(shorter  wavelength)  electromagnetic  phenomena  carry  more  energy  per  photon.  "
"Thus,  radio  waves  have  photons  with  low  energies,  microwaves  have  more  energy  "
"than  radio  waves,  infrared  still  more,  then  visible,  ultraviolet,  X-rays,  and  finally  "
"gamma rays, the most energetic of all. High-energy electromagnetic radiation, espe-"
"cially in the X-ray and gamma ray bands, is particularly harmful to living organisms. "
"Light  is  a  type  of  electromagnetic  radiation  that  can  be  sensed  by  the  eye.  The  "
"visible (color) spectrum is shown expanded in Fig. 2.10 for the purpose of discussion "
"(we  will  discuss  color  in  detail  in  Chapter  6).  The  visible  band  of  the  electromag-"
"netic spectrum spans the range from approximately 0.43 mm  (violet) to about 0.79 "
"mm   (red).  For  convenience,  the  color  spectrum  is  divided  into  six  broad  regions:  "
"violet,  blue,  green,  yellow,  orange,  and  red.  No  color  (or  other  component  of  the  "
"Radio waves","Microwaves","Infrared"
"Visible spectrum"
"Ultraviolet","Gamma rays   X-rays"
"0.4  10"
"6"
"0.5  10"
"6"
"0.6  10"
"6"
"0.7  10"
"6"
"Infrared","Ultraviolet   Violet","Blue","Green    Yellow","Red","Orange"
"10"
"5"
"10"
"6"
"10"
"7"
"10"
"8"
"10"
"9"
"10"
"10"
"10"
"11"
"10"
"12"
"10"
"13"
"10"
"14"
"10"
"15"
"10"
"16"
"10"
"17"
"10"
"18"
"10"
"19"
"10"
"20"
"10"
"21"
"Frequency (Hz)"
"10"
"9"
"10"
"8"
"10"
"7"
"10"
"6"
"10"
"5"
"10"
"4"
"10"
"3"
"10"
"2"
"10"
"1"
"1"
"10"
"1"
"10"
"2"
"10"
"3"
"10"
"4"
"10"
"5"
"10"
"6"
"Energy of one photon (electron volts)"
"10"
"3"
"10"
"2"
"10"
"1"
"1"
"10"
"1"
"10"
"2"
"10"
"3"
"10"
"4"
"10"
"5"
"10"
"6"
"10"
"7"
"10"
"8"
"10"
"9"
"10"
"10"
"10"
"11"
"10"
"12"
"Wavelength (meters)"
"FIGURE 2.10"
"  The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanations, but note "
"that it encompasses a very narrow range of the total EM spectrum."
"l"
"FIGURE 2.11"
"Graphical  "
"representation of "
"one wavelength."
"DIP","4E_GLOBAL_Print_Ready.indb   566/16/2017   2:02:06 PM"
"www.EBooks","World.ir"
""
"2.3"
"  "
"Image Sensing and Acquisition"
"    "
"57"
"electromagnetic spectrum) ends abruptly; rather, each range blends smoothly into "
"the next, as Fig. 2.10 shows."
"The colors perceived in an object are determined by the nature of the light reflect-"
"ed  by  the  object.  A  body  that  reflects  light  relatively  balanced  in  all  visible  wave-"
"lengths  appears  white  to  the  observer.  However,  a  body  that  favors  reflectance  in  "
"a limited range of the visible spectrum exhibits some shades of color. For example, "
"green  objects  reflect  light  with  wavelengths  primarily  in  the  500  to  570  nm  range,  "
"while absorbing most of the energy at other wavelengths."
"Light  that  is  void  of  color  is  called  monochromatic  (or  achromatic)  light.  The  "
"only attribute of monochromatic light is its intensity. Because the intensity of mono-"
"chromatic  light  is  perceived  to  vary  from  black  to  grays  and  finally  to  white,  the  "
"term gray  level  is  used  commonly  to  denote  monochromatic  intensity  (we  use  the  "
"terms intensity and gray level interchangeably in subsequent discussions). The range "
"of values of monochromatic light from black to white is usually called the gray scale, "
"and monochromatic images are frequently referred to as grayscale images."
"Chromatic (color) light spans the electromagnetic energy spectrum from approxi-"
"mately  0.43  to  0.79  mm,  as  noted  previously.  In  addition  to  frequency,  three  other  "
"quantities  are  used  to  describe  a  chromatic  light  source:  radiance,  luminance,  and  "
"brightness. Radiance is the total amount of energy that flows from the light source, "
"and it is usually measured in watts (W). Luminance, measured in lumens (lm), gives "
"a  measure  of  the  amount  of  energy  an  observer  perceives  from  a  light  source.  For  "
"example,  light  emitted  from  a  source  operating  in  the  far  infrared  region  of  the  "
"spectrum  could  have  significant  energy  (radiance),  but  an  observer  would  hardly  "
"perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, "
"brightness is a subjective descriptor of light perception that is practically impossible "
"to measure. It embodies the achromatic notion of intensity and is one of the key fac-"
"tors in describing color sensation."
"In  principle,  if  a  sensor  can  be  developed  that  is  capable  of  detecting  energy  "
"radiated  in  a  band  of  the  electromagnetic  spectrum,  we  can  image  events  of  inter-"
"est  in  that  band.  Note,  however,  that  the  wavelength  of  an  electromagnetic  wave  "
"required to “see” an object must be of the same size as, or smaller than, the object. "
"For example, a water molecule has a diameter on the order of 10"
"10−"
" m. Thus, to study "
"these molecules, we would need a source capable of emitting energy in the far (high-"
"energy) ultraviolet band or soft (low-energy) X-ray bands. "
"Although imaging is based predominantly on energy from electromagnetic wave "
"radiation, this is not the only method for generating images. For example, we saw in "
"Section 1.3 that sound reflected from objects can be used to form ultrasonic images. "
"Other  sources  of  digital  images  are  electron  beams  for  electron  microscopy,  and  "
"software for generating synthetic images used in graphics and visualization."
"2.3 IMAGE SENSING AND ACQUISITION  "
"Most of the images in which we are interested are generated by the combination of "
"an “illumination” source and the reflection or absorption of energy from that source "
"by  the  elements  of  the  “scene”  being  imaged.  We  enclose  illumination  and  scene "
"in  quotes  to  emphasize  the  fact  that  they  are  considerably  more  general  than  the  "
"2.3"
"DIP","4E_GLOBAL_Print_Ready.indb   576/16/2017   2:02:06 PM"
"www.EBooks","World.ir"
""
"58"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"familiar situation in which a visible light source illuminates a familiar 3-D scene. For "
"example,  the  illumination  may  originate  from  a  source  of  electromagnetic  energy,  "
"such  as  a  radar,  infrared,  or  X-ray  system.  But,  as  noted  earlier,  it  could  originate  "
"from less traditional sources, such as ultrasound or even a computer-generated illu-"
"mination  pattern.  Similarly,  the  scene  elements  could  be  familiar  objects,  but  they  "
"can just as easily be molecules, buried rock formations, or a human brain. Depend-"
"ing on the nature of the source, illumination energy is reflected from, or transmitted "
"through,  objects.  An  example  in  the  first  category  is  light  reflected  from  a  planar  "
"surface. An example in the second category is when X-rays pass through a patient’s "
"body for the purpose of generating a diagnostic X-ray image. In some applications, "
"the reflected or transmitted energy is focused onto a photo converter (e.g., a phos-"
"phor  screen)  that  converts  the  energy  into  visible  light.  Electron  microscopy  and  "
"some applications of gamma imaging use this approach. "
"Figure 2.12 shows the three principal sensor arrangements used to transform inci-"
"dent energy into digital images. The idea is simple: Incoming energy is transformed "
"into  a  voltage  by  a  combination  of  the  input  electrical  power  and  sensor  material  "
"that  is  responsive  to  the  type  of  energy  being  detected.  The  output  voltage  wave-"
"form is the response of the sensor, and a digital quantity is obtained by digitizing that "
"response. In this section, we look at the principal modalities for image sensing and "
"generation. We will discuss image digitizing in Section 2.4."
"IMAGE ACQUISITION USING A SINGLE SENSING ELEMENT"
"Figure 2.12(a) shows the components of a single sensing element. A familiar sensor "
"of this type is the photodiode, which is constructed of silicon materials and whose "
"output is a voltage proportional to light intensity. Using a filter in front of a sensor "
"improves its selectivity. For example, an optical green-transmission filter favors light "
"in the green band of the color spectrum. As a consequence, the sensor output would "
"be stronger for green light than for other visible light components."
"In  order  to  generate  a  2-D  image  using  a  single  sensing  element,  there  has  to  "
"be  relative  displacements  in  both  the  x-  and  y-directions  between  the  sensor  and  "
"the  area  to  be  imaged.  Figure  2.13  shows  an  arrangement  used  in  high-precision  "
"scanning, where a film negative is mounted onto a drum whose mechanical rotation "
"provides  displacement  in  one  dimension.  The  sensor  is  mounted  on  a  lead  screw  "
"that  provides  motion  in  the  perpendicular  direction.  A  light  source  is  contained  "
"inside  the  drum.  As  the  light  passes  through  the  film,  its  intensity  is  modified  by  "
"the film density before it is captured by the sensor. This "modulation" of the light "
"intensity causes corresponding variations in the sensor voltage, which are ultimately "
"converted to image intensity levels by digitization. "
"This  method  is  an  inexpensive  way  to  obtain  high-resolution  images  because  "
"mechanical motion can be controlled with high precision. The main disadvantages "
"of this method are that it is slow and not readily portable. Other similar mechanical "
"arrangements  use  a  flat  imaging  bed,  with  the  sensor  moving  in  two  linear  direc-"
"tions. These types of mechanical digitizers sometimes are referred to as transmission "
"microdensitometers.  Systems  in  which  light  is  reflected  from  the  medium,  instead  "
"of  passing  through  it,  are  called  reflection  microdensitometers.  Another  example  "
"of imaging with a single sensing element places a laser source coincident with the "
"DIP","4E_GLOBAL_Print_Ready.indb   586/16/2017   2:02:06 PM"
"www.EBooks","World.ir"
""
"2.3"
"  "
"Image Sensing and Acquisition"
"59"
"Sensing material"
"Voltage waveform out"
"Filter"
"Energy"
"Power in"
"Housing"
"b"
"a"
"c"
"FIGURE 2.12"
"(a) Single sensing "
"element. "
"(b) Line sensor.  "
"(c) Array sensor."
"Sensor"
"Linear motion"
"One image line out"
"per increment of rotation"
"and full linear displacement"
"of sensor from left to right"
"Film"
"Rotation"
"FIGURE 2.13"
"Combining a "
"single sensing "
"element with "
"mechanical  "
"motion to  "
"generate a 2-D "
"image."
"DIP","4E_GLOBAL_Print_Ready.indb   596/16/2017   2:02:07 PM"
"www.EBooks","World.ir"
""
"60"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"sensor. Moving mirrors are used to control the outgoing beam in a scanning pattern "
"and to direct the reflected laser signal onto the sensor. "
"IMAGE ACQUISITION USING SENSOR STRIPS"
"A geometry used more frequently than single sensors is an in-line sensor strip, as in "
"Fig.  2.12(b).  The  strip  provides  imaging  elements  in  one  direction.  Motion  perpen-"
"dicular to the strip provides imaging in the other direction, as shown in Fig. 2.14(a). "
"This  arrangement  is  used  in  most  flat  bed  scanners.  Sensing  devices  with  4000  or  "
"more  in-line  sensors  are  possible.  In-line  sensors  are  used  routinely  in  airborne  "
"imaging  applications,  in  which  the  imaging  system  is  mounted  on  an  aircraft  that  "
"flies at a constant altitude and speed over the geographical area to be imaged. One-"
"dimensional imaging sensor strips that respond to various bands of the electromag-"
"netic  spectrum  are  mounted  perpendicular  to  the  direction  of  flight.  An  imaging  "
"strip  gives  one  line  of  an  image  at  a  time,  and  the  motion  of  the  strip  relative  to  "
"the scene completes the other dimension of a 2-D image. Lenses or other focusing "
"schemes are used to project the area to be scanned onto the sensors."
"Sensor strips in a ring configuration are used in medical and industrial imaging "
"to  obtain  cross-sectional  (“slice”)  images  of  3-D  objects,  as  Fig.  2.14(b)  shows.  A  "
"rotating  X-ray  source  provides  illumination,  and  X-ray  sensitive  sensors  opposite  "
"the  source  collect  the  energy  that  passes  through  the  object.  This  is  the  basis  for  "
"medical and industrial computerized axial tomography (CAT) imaging, as indicated "
"in  Sections  1.2  and  1.3.  The  output  of  the  sensors  is  processed  by  reconstruction  "
"algorithms  whose  objective  is  to  transform  the  sensed  data  into  meaningful  cross-"
"sectional images (see Section 5.11). In other words, images are not obtained directly "
"Sensor strip"
"Linear "
"motion"
"Imaged area"
"One image line out per"
"increment of linear motion"
"Image"
"reconstruction"
"3-D object"
"Linear motion"
"Sensor ring"
"X-ray source"
"Cross-sectional images"
"of 3-D object"
"Source"
"rotation"
"ba"
"FIGURE 2.14"
"(a) Image  "
"acquisition using "
"a linear sensor "
"strip. (b) Image "
"acquisition using "
"a circular sensor "
"strip."
"DIP","4E_GLOBAL_Print_Ready.indb   606/16/2017   2:02:07 PM"
"www.EBooks","World.ir"
""
"2.3"
"  "
"Image Sensing and Acquisition"
"    "
"61"
"from  the  sensors  by  motion  alone;  they  also  require  extensive  computer  process-"
"ing. A 3-D digital volume consisting of stacked images is generated as the object is "
"moved in a direction perpendicular to the sensor ring. Other modalities of imaging "
"based  on  the  CAT  principle  include  magnetic  resonance  imaging  (MRI)  and  posi-"
"tron  emission  tomography  (PET).  The  illumination  sources,  sensors,  and  types  of  "
"images are different, but conceptually their applications are very similar to the basic "
"imaging approach shown in Fig. 2.14(b)."
"IMAGE ACQUISITION USING SENSOR ARRAYS"
"Figure 2.12(c) shows individual sensing elements arranged in the form of a 2-D array. "
"Electromagnetic and ultrasonic sensing devices frequently are arranged in this man-"
"ner.  This  is  also  the  predominant  arrangement  found  in  digital  cameras.  A  typical  "
"sensor  for  these  cameras  is  a  CCD  (charge-coupled  device)  array,  which  can  be  "
"manufactured with a broad range of sensing properties and can be packaged in rug-"
"ged arrays of 40004000* elements or more. CCD sensors are used widely in digital "
"cameras  and  other  light-sensing  instruments.  The  response  of  each  sensor  is  pro-"
"portional to the integral of the light energy projected onto the surface of the sensor, "
"a property that is used in astronomical and other applications requiring low noise "
"images.  Noise  reduction  is  achieved  by  letting  the  sensor  integrate  the  input  light  "
"signal  over  minutes  or  even  hours.  Because  the  sensor  array  in  Fig.  2.12(c)  is  two-"
"dimensional, its key advantage is that a complete image can be obtained by focusing "
"the energy pattern onto the surface of the array. Motion obviously is not necessary, "
"as is the case with the sensor arrangements discussed in the preceding two sections."
"Figure  2.15  shows  the  principal  manner  in  which  array  sensors  are  used.  This  "
"figure  shows  the  energy  from  an  illumination  source  being  reflected  from  a  scene  "
"(as  mentioned  at  the  beginning  of  this  section,  the  energy  also  could  be  transmit-"
"ted through the scene). The first function performed by the imaging system in Fig. "
"2.15(c) is to collect the incoming energy and focus it onto an image plane. If the illu-"
"mination is light, the front end of the imaging system is an optical lens that projects "
"the viewed scene onto the focal plane of the lens, as Fig. 2.15(d) shows. The sensor "
"array, which is coincident with the focal plane, produces outputs proportional to the "
"integral of the light received at each sensor. Digital and analog circuitry sweep these "
"outputs and convert them to an analog signal, which is then digitized by another sec-"
"tion of the imaging system. The output is a digital image, as shown diagrammatically "
"in Fig. 2.15(e). Converting images into digital form is the topic of Section 2.4."
"A SIMPLE IMAGE FORMATION MODEL"
"As introduced in Section 1.1, we denote images by two-dimensional functions of the "
"form fxy(,). The value of f  at  spatial  coordinates  (, )","xy  is  a  scalar  quantity  whose  "
"physical  meaning  is  determined  by  the  source  of  the  image,  and  whose  values  are  "
"proportional to energy radiated by a physical source (e.g., electromagnetic waves). "
"As a consequence, fxy(,) must be nonnegative"
"†"
" and finite; that is,"
"†"
"  Image intensities can become negative during processing, or as a result of interpretation. For example, in radar "
"images, objects moving toward the radar often are interpreted as having negative velocities while objects moving "
"away are interpreted as having positive velocities. Thus, a velocity image might be coded as having both positive "
"and negative values. When storing and displaying images, we normally scale the intensities so that the smallest "
"negative value becomes 0 (see Section 2.6 regarding intensity scaling)."
"In some cases, the source "
"is imaged directly, as "
"in obtaining images of "
"the sun."
"DIP","4E_GLOBAL_Print_Ready.indb   616/16/2017   2:02:07 PM"
"www.EBooks","World.ir"
""
"62"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
" 0≤<fxy(, ) "
"(2-3)"
"Function fxy(,) is characterized by two components: (1) the amount of source illu-"
"mination  incident  on  the  scene  being  viewed,  and  (2)  the  amount  of  illumination  "
"reflected by the objects in the scene. Appropriately, these are called the illumination "
"and reflectance components, and are denoted by ixy(,) and rxy(,), respectively. The "
"two functions combine as a product to form fxy(,):"
" fxy    ixyrxy(,)   (, )(, )= "
"(2-4)"
"where"
" 0≤<ixy(, ) "
"(2-5)"
"and"
" 01≤≤rxy(, ) "
"(2-6)"
"Thus,  reflectance  is  bounded  by  0  (total  absorption)  and  1  (total  reflectance).  The  "
"nature of ixy(,) is determined by the illumination source, and rxy(,) is determined "
"by  the  characteristics  of  the  imaged  objects.  These  expressions  are  applicable  also  "
"to images formed via transmission of the illumination through a medium, such as a "
"Illumination (energy)"
"source"
"Imaging system"
"(Internal) image plane"
"Output (digitized) image"
"Scene"
"b"
"a"
"d"
"ce"
"FIGURE  2.15"
"    An  example  of  digital  image  acquisition.  (a)  Illumination  (energy)  source.  (b)  A  scene.  (c)  Imaging  "
"system. (d) Projection of the scene onto the image plane. (e) Digitized image."
"DIP","4E_GLOBAL_Print_Ready.indb   626/16/2017   2:02:08 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"63"
"chest X-ray. In this case, we would deal with a transmissivity instead of a reflectivity "
"function, but the limits would be the same as in Eq. (2-6), and the image function "
"formed would be modeled as the product in Eq. (2-4)."
"EXAMPLE 2.1 :  Some typical values of illumination and reflectance."
"The  following  numerical  quantities  illustrate  some  typical  values  of  illumination  and  reflectance  for  "
"visible light. On a clear day, the sun may produce in excess of 90 000, lm/m"
"2"
" of illumination on the sur-"
"face of the earth. This value decreases to less than 10 000, lm/m"
"2"
" on a cloudy day. On a clear evening, a "
"full moon yields about 01.   lm/m"
"2"
" of illumination. The typical illumination level in a commercial office "
"is about 1 000, lm/m"
"2"
". Similarly, the following are typical values of rxy(,): 0.01 for black velvet, 0.65 for "
"stainless steel, 0.80 for flat-white wall paint, 0.90 for silver-plated metal, and 0.93 for snow. "
"Let  the  intensity  (gray  level)  of  a  monochrome  image  at  any  coordinates  (, )","xy "
"be denoted by "
" /=fxy(, ) "
"(2-7)"
"From Eqs. (2-4) through (2-6) it is evident that / lies in the range"
" LL"
"minmax"
"≤≤/ "
"(2-8)"
"In  theory,  the  requirement  on  L"
"min"
"  is  that  it  be  nonnegative,  and  on  L"
"max"
"  that  it  "
"be  finite.  In  practice,  Lir"
"minmin   min"
"=  and  Lir"
"maxmax   max"
"=.  From  Example  2.1,  using  "
"average  office  illumination  and  reflectance  values  as  guidelines,  we  may  expect  "
"L"
"min"
"≈10 and L"
"ma x"
"≈1000 to be typical indoor values in the absence of additional "
"illumination.  The  units  of  these  quantities  are  lum/m"
"2"
".  However,  actual  units  sel-"
"dom  are  of  interest,  except  in  cases  where  photometric  measurements  are  being  "
"performed."
"The interval [, ]"
"minmax"
"LL is called the intensity (or gray) scale. Common practice is "
"to shift this interval numerically to the interval [, ],01 or [, ],0C where /=0 is consid-"
"ered black and /=1 (or )","C is considered white on the scale. All intermediate values "
"are shades of gray varying from black to white."
"2.4  IMAGE SAMPLING AND QUANTIZATION  "
"As discussed in the previous section, there are numerous ways to acquire images, but "
"our objective in all is the same: to generate digital images from sensed data. The out-"
"put of most sensors is a continuous voltage waveform whose amplitude and spatial "
"behavior are related to the physical phenomenon being sensed. To create a digital "
"image,  we  need  to  convert  the  continuous  sensed  data  into  a  digital  format.  This  "
"requires two processes: sampling and quantization."
"BASIC CONCEPTS IN SAMPLING AND QUANTIZATION"
"Figure 2.16(a) shows a continuous image f that we want to convert to digital form. "
"An image may be continuous with respect to the x- and y-coordinates, and also in "
"2.4"
"The discussion of sam-"
"pling in this section is of "
"an intuitive nature. We "
"will discuss this topic in "
"depth in Chapter 4."
"DIP","4E_GLOBAL_Print_Ready.indb   636/16/2017   2:02:10 PM"
"www.EBooks","World.ir"
""
"64"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"amplitude.  To  digitize  it,  we  have  to  sample  the  function  in  both  coordinates  and  "
"also in amplitude. Digitizing the coordinate values is called sampling. Digitizing the "
"amplitude values is called quantization."
"The  one-dimensional  function  in  Fig.  2.16(b)  is  a  plot  of  amplitude  (intensity  "
"level) values of the continuous image along the line segment AB in Fig. 2.16(a). The "
"random variations are due to image noise. To sample this function, we take equally "
"spaced samples along line AB, as shown in Fig. 2.16(c). The samples are shown as "
"small dark squares superimposed on the function, and their (discrete) spatial loca-"
"tions are indicated by corresponding tick marks in the bottom of the figure. The set "
"of  dark  squares  constitute  the  sampled  function.  However,  the  values  of  the  sam-"
"ples still span (vertically) a continuous range of intensity values. In order to form a "
"digital function, the intensity values also must be converted (quantized) into discrete "
"quantities.  The  vertical  gray  bar  in  Fig.  2.16(c)  depicts  the  intensity  scale  divided  "
"into  eight  discrete  intervals,  ranging  from  black  to  white.  The  vertical  tick  marks  "
"indicate the specific value assigned to each of the eight intensity intervals. The con-"
"tinuous intensity levels are quantized by assigning one of the eight values to each "
"sample, depending on the vertical proximity of a sample to a vertical tick mark. The "
"digital samples resulting from both sampling and quantization are shown as white "
"squares in Fig. 2.16(d). Starting at the top of the continuous image and carrying out "
"this  procedure  downward,  line  by  line,  produces  a  two-dimensional  digital  image.  "
"It is implied in Fig. 2.16 that, in addition to the number of discrete levels used, the "
"accuracy achieved in quantization is highly dependent on the noise content of the "
"sampled signal. "
"ba"
"dc"
"FIGURE 2.16"
"(a) Continuous "
"image. (b) A "
"scan line show-"
"ing intensity "
"variations along "
"line AB in the "
"continuous image. "
"(c) Sampling and "
"quantization.  "
"(d) Digital scan "
"line. (The black "
"border in (a) is "
"included for  "
"clarity. It is not "
"part of the image)."
"AB"
"AB"
"Sampling"
"ABAB"
"Quantization"
"DIP","4E_GLOBAL_Print_Ready.indb   646/16/2017   2:02:10 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"65"
"In  practice,  the  method  of  sampling  is  determined  by  the  sensor  arrangement  "
"used to generate the image. When an image is generated by a single sensing element "
"combined with mechanical motion, as in Fig. 2.13, the output of the sensor is quan-"
"tized in the manner described above. However, spatial sampling is accomplished by "
"selecting the number of individual mechanical increments at which we activate the "
"sensor to collect data. Mechanical motion can be very exact so, in principle, there is "
"almost no limit on how fine we can sample an image using this approach. In practice, "
"limits on sampling accuracy are determined by other factors, such as the quality of "
"the optical components used in the system."
"When a sensing strip is used for image acquisition, the number of sensors in the "
"strip establishes the samples in the resulting image in one direction, and mechanical "
"motion establishes the number of samples in the other. Quantization of the sensor "
"outputs completes the process of generating a digital image."
"When  a  sensing  array  is  used  for  image  acquisition,  no  motion  is  required.  The  "
"number of sensors in the array establishes the limits of sampling in both directions. "
"Quantization of the sensor outputs is as explained above. Figure 2.17 illustrates this "
"concept. Figure 2.17(a) shows a continuous image projected onto the plane of a 2-D "
"sensor. Figure 2.17(b) shows the image after sampling and quantization. The quality "
"of a digital image is determined to a large degree by the number of samples and dis-"
"crete intensity levels used in sampling and quantization. However, as we will show "
"later in this section, image content also plays a role in the choice of these parameters."
"REPRESENTING DIGITAL IMAGES"
"Let fst(,) represent a continuous image function of two continuous variables, s and "
"t.  We  convert  this  function  into  a  digital  image  by  sampling  and  quantization,  as  "
"explained  in  the  previous  section.  Suppose  that  we  sample  the  continuous  image  "
"into  a  digital  image,  fxy(,),  containing  M  rows  and  N  columns,  where  (, )","xy  are  "
"discrete coordinates. For notational clarity and convenience, we use integer values "
"for  these  discrete  coordinates:  x","M=−0121,, ,  ,...  and  y","N=−0121,, ,  ,....  Thus,  "
"for  example,  the  value  of  the  digital  image  at  the  origin  is  f(,)00,  and  its  value  at  "
"the next coordinates along the first row is f(,)01. Here, the notation (0, 1) is used "
"ba"
"FIGURE 2.17"
"(a) Continuous "
"image projected "
"onto a sensor "
"array. (b) Result "
"of image sampling "
"and quantization."
"DIP","4E_GLOBAL_Print_Ready.indb   656/16/2017   2:02:10 PM"
"www.EBooks","World.ir"
""
"66"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"to  denote  the  second  sample  along  the  first  row.  It  does  not  mean  that  these  are  "
"the values of the physical coordinates when the image was sampled. In general, the "
"value of a digital image at any coordinates (, )","xy is denoted fxy(,), where x and y "
"are integers. When we need to refer to specific coordinates (, )","ij, we use the notation "
"fij(,),  where the arguments are integers. The section of the real plane spanned by "
"the coordinates of an image is called the spatial domain, with x and y being referred "
"to as spatial variables or spatial coordinates."
"Figure  2.18  shows  three  ways  of  representing  fxy(,).  Figure  2.18(a)  is  a  plot  of  "
"the function, with two axes determining spatial location and the third axis being the "
"values of f as a function of x and y. This representation is useful when working with "
"grayscale sets whose elements are expressed as triplets of the form (, ,)","xyz, where "
"x and y are spatial coordinates and z is the value of f at coordinates (, ).xy We will "
"work with this representation briefly in Section 2.6."
"The representation in Fig. 2.18(b) is more common, and it shows fxy(,) as it would "
"appear on a computer display or photograph. Here, the intensity of each point in the "
"display is proportional to the value of f at that point. In this figure, there are only "
"three  equally  spaced  intensity  values.  If  the  intensity  is  normalized  to  the  interval  "
"[,],01 then each point in the image has the value 0, 0.5, or 1. A monitor or printer con-"
"verts these three values to black, gray, or white, respectively, as in Fig. 2.18(b). This "
"type of representation includes color images, and allows us to view results at a glance."
"As Fig. 2.18(c) shows, the third representation is an array (matrix) composed of "
"the numerical values of fxy(,). This is the representation used for computer process-"
"ing. In equation form, we write the representation of an MN* numerical array as"
" fxy"
"fff","N"
"fff","N"
"f","M"
"(, )"
"(, )(,)(,)"
"(, )(, )(,)"
"(,"
"="
"−"
"−"
"−"
"000101"
"101111"
"1"
""
""
"   "
"001111)(    ,)(    ,    )","f","Mf","MN−−−"
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"⎥"
""
" "
"(2-9)"
"The  right  side  of  this  equation  is  a  digital  image  represented  as  an  array  of  real  "
"numbers. Each element of this array is called an image element, picture element, pixel, "
"or pel.  We  use  the  terms  image  and  pixel  throughout  the  book  to  denote  a  digital  "
"image  and  its  elements.  Figure  2.19  shows  a  graphical  representation  of  an  image  "
"array, where the x- and y-axis are used to denote the rows and columns of the array. "
"Specific pixels are values of the array at a fixed pair of coordinates. As mentioned "
"earlier, we generally use fij(,) when referring to a pixel with coordinates (, ).ij"
"We can also represent a digital image in a traditional matrix form:"
" A="
"⎡"
"⎣"
"−"
"−"
"−−−−"
"aaa"
"aaa"
"aaa"
"N"
"N"
"MMMN"
"000101"
"101111"
"101111"
",,,"
",,,"
",,,"
""
""
"  "
""
"⎢⎢"
"⎢"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"⎥"
"⎥"
" "
"(2-10)"
"Clearly, afij"
"ij"
"=(, ), so Eqs. (2-9) and (2-10) denote identical arrays."
"DIP","4E_GLOBAL_Print_Ready.indb   666/16/2017   2:02:12 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"67"
"As Fig. 2.19 shows, we define the origin of an image at the top left corner. This is "
"a convention based on the fact that many image displays (e.g., TV monitors) sweep "
"an image starting at the top left and moving to the right, one row at a time. More "
"important is the fact that the first element of a matrix is by convention at the top "
"left of the array. Choosing the origin of fxy(,) at that point makes sense mathemati-"
"cally because digital images in reality are matrices. In fact, as you will see, sometimes "
"we use x and y interchangeably in equations with the rows (r) and columns (c) of a "
"matrix."
"It is important to note that the representation in Fig. 2.19, in which the positive "
"x-axis extends downward and the positive y-axis extends to the right, is precisely the "
"right-handed Cartesian coordinate system with which you are familiar,"
"†"
" but shown "
"rotated by 90° so that the origin appears on the top, left."
"†"
" Recall that a right-handed coordinate system is such that, when the index of the right hand points in the direc-"
"tion of the positive x-axis and the middle finger points in the (perpendicular) direction of the positive y-axis, the "
"thumb points up. As Figs. 2.18 and 2.19 show, this indeed is the case in our image coordinate system. In practice, "
"you will also find implementations based on a left-handed system, in which the x- and y-axis are interchanged "
"from the way we show them in Figs. 2.18 and 2.19. For example, MATLAB uses a left-handed system for image "
"processing. Both systems are perfectly valid, provided they are used consistently."
"x"
"y"
"f(x, y)"
".5"
"y"
"x"
"Origin"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"1"
"1"
"11"
"1"
".5"
".5"
".5"
".5"
".5"
".5"
"b"
"a"
"c"
"FIGURE 2.18"
"(a) Image plotted "
"as a surface.  "
"(b) Image displayed "
"as a visual intensity "
"array. (c) Image "
"shown as a 2-D nu-"
"merical array. (The "
"numbers 0, .5, and "
"1 represent black, "
"gray, and white, "
"respectively.)"
"DIP","4E_GLOBAL_Print_Ready.indb   676/16/2017   2:02:16 PM"
"www.EBooks","World.ir"
""
"68"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"The center of an MN× digital image with origin at (, )00 and range to (,)","MN−−11"
"is  obtained  by  dividing  M  and  N  by  2  and  rounding  down  to  the  nearest  integer.  "
"This  operation  sometimes  is  denoted  using  the  floor  operator,  JKi,  as  shown  in  Fig.  "
"2.19. This holds true for M and N even or odd. For example, the center of an image "
"of size 1023    1024× is at (, ).511512 Some programming languages (e.g., MATLAB) "
"start  indexing  at  1  instead  of  at  0.  The  center  of  an  image  in  that  case  is  found  at  "
"(, )()  ,(    )    .xy","MN"
"cc"
"=+ +"
"()"
"floorfloor","2121"
"To  express  sampling  and  quantization  in  more  formal  mathematical  terms,  let  "
"Z  and  R  denote  the  set  of  integers  and  the  set  of  real  numbers,  respectively.  The  "
"sampling  process  may  be  viewed  as  partitioning  the  xy-plane  into  a  grid,  with  the  "
"coordinates of the center of each cell in the grid being a pair of elements from the "
"Cartesian  product  Z"
"2"
"  (also  denoted  ZZ×)  which,  as  you  may  recall,  is  the  set  of  "
"all ordered pairs of elements (, )","zz"
"ij"
" with z"
"i"
" and z"
"j"
" being integers from set Z. Hence, "
"fxy(,) is a digital image if (, )","xy are integers from Z"
"2"
" and f is a function that assigns "
"an intensity value (that is, a real number from the set of real numbers, R) to each "
"distinct pair of coordinates (, )","xy. This functional assignment is the quantization pro-"
"cess  described  earlier.  If  the  intensity  levels  also  are  integers,  then","RZ=,  and  a  "
"digital image becomes a 2-D function whose coordinates and amplitude values are "
"integers. This is the representation we use in the book."
"Image digitization requires that decisions be made regarding the values for M, N, "
"and for the number, L, of discrete intensity levels. There are no restrictions placed "
"on M and N, other than they have to be positive integers. However, digital storage "
"and quantizing hardware considerations usually lead to the number of intensity lev-"
"els, L, being an integer power of two; that is"
"L"
"k"
"=2"
"(2-11)"
"where k is an integer. We assume that the discrete levels are equally spaced and that "
"they are integers in the range [,]01L−. "
"The floor of z, sometimes "
"denoted Jz","K, is the largest "
"integer that is less than "
"or equal to z. The ceiling "
"of z, denoted Lz","M, is the "
"smallest integer that is "
"greater than or equal "
"to z."
"See Eq. (2-41) in  "
"Section 2.6 for a formal "
"definition of the  "
"Cartesian product."
"FIGURE 2.19"
"Coordinate  "
"convention used "
"to represent digital "
"images. Because "
"coordinate values "
"are integers, there "
"is a one-to-one "
"correspondence "
"between x and y "
"and the rows (r) "
"and columns (c) of "
"a matrix."
"Origin"
"0"
"N "
"-"
"1"
"-1M"
"0"
"y"
"x"
"i"
"j "
"pixel f(i, j)"
"Image f(x, y)"
"1"
"1"
"2"
"Center"
"The coordinates of the "
"image center are"
"x"
"c"
"y"
"c"
"x"
"c"
", y"
"c"
"  =  "
"N"
"2"
"QR"
"floor"
"M"
"2"
"QR"
"floor,"
"a"
"b"
"BA"
"DIP","4E_GLOBAL_Print_Ready.indb   686/16/2017   2:02:18 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"69"
"Sometimes,  the  range  of  values  spanned  by  the  gray  scale  is  referred  to  as  the  "
"dynamic range, a term used in different ways in different fields. Here, we define the "
"dynamic  range  of  an  imaging  system  to  be  the  ratio  of  the  maximum  measurable  "
"intensity  to  the  minimum  detectable  intensity  level  in  the  system.  As  a  rule,  the  "
"upper limit is determined by saturation and the lower limit by noise, although noise "
"can be present also in lighter intensities. Figure 2.20 shows examples of saturation "
"and slight visible noise. Because the darker regions are composed primarily of pixels "
"with  the  minimum  detectable  intensity,  the  background  in  Fig.  2.20  is  the  noisiest  "
"part of the image; however, dark background noise typically is much harder to see. "
"The dynamic range establishes the lowest and highest intensity levels that a system "
"can represent and, consequently, that an image can have. Closely associated with this "
"concept  is  image  contrast,  which  we  define  as  the  difference  in  intensity  between  "
"the highest and lowest intensity levels in an image. The contrast ratio is the ratio of "
"these two quantities. When an appreciable number of pixels in an image have a high "
"dynamic range, we can expect the image to have high contrast. Conversely, an image "
"with low dynamic range typically has a dull, washed-out gray look. We will discuss "
"these concepts in more detail in Chapter 3."
"The number, b, of bits required to store a digital image is"
" b","MNk=** "
"(2-12)"
"When MN=, this equation becomes"
" b","Nk="
"2"
" "
"(2-13)"
"Noise"
"Saturation"
"FIGURE 2.20"
"An image exhibit-"
"ing saturation and "
"noise. Saturation "
"is the highest val-"
"ue beyond which "
"all intensity values "
"are clipped (note "
"how the entire "
"saturated area has "
"a high, constant "
"intensity level). "
"Visible noise in "
"this case appears "
"as a grainy texture "
"pattern. The dark "
"background is "
"noisier, but the "
"noise is difficult "
"to see."
"DIP","4E_GLOBAL_Print_Ready.indb   696/16/2017   2:02:18 PM"
"www.EBooks","World.ir"
""
"70"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"Figure  2.21  shows  the  number  of  megabytes  required  to  store  square  images  for  "
"various values of N and k (as usual, one byte equals 8 bits and a megabyte equals "
"10"
"6"
" bytes). "
"When  an  image  can  have  2"
"k"
"  possible  intensity  levels,  it  is  common  practice  to  "
"refer to it as a “k-bit image,” (e,g., a 256-level image is called an 8-bit image). Note "
"that  storage  requirements  for  large  8-bit  images  (e.g.,  10 00010 000,,*  pixels)  are  "
"not insignificant."
"LINEAR VS. COORDINATE INDEXING"
"The convention discussed in the previous section, in which the location of a pixel is "
"given by its 2-D coordinates, is referred to as coordinate indexing, or subscript index-"
"ing.  Another  type  of  indexing  used  extensively  in  programming  image  processing  "
"algorithms is linear indexing, which consists of a 1-D string of nonnegative integers "
"based on computing offsets from coordinates (, )00. There are two principal types of "
"linear indexing, one is based on a row scan of an image, and the other on a column scan."
"Figure  2.22  illustrates  the  principle  of  linear  indexing  based  on  a  column  scan.  "
"The idea is to scan an image column by column, starting at the origin and proceeding "
"down and then to the right. The linear index is based on counting pixels as we scan "
"the image in the manner shown in Fig. 2.22. Thus, a scan of the first (leftmost) column "
"yields linear indices 0 through M−1. A scan of the second column yields indices M "
"through 21M−, and so on, until the last pixel in the last column is assigned the linear "
"index  value  MN−1.  Thus,  a  linear  index,  denoted  by  a,  has  one  of  MN  possible  "
"values: 0121,,,  ,...MN−, as Fig. 2.22 shows. The important thing to notice here is "
"that each pixel is assigned a linear index value that identifies it uniquely."
"The formula for generating linear indices based on a column scan is straightfor-"
"ward  and  can  be  determined  by  inspection.  For  any  pair  of  coordinates  (, )","xy,  the  "
"corresponding linear index value is"
" a=+Myx "
"(2-14)"
"N"
"*"
"10"
"3"
"10"
"20"
"30"
"40"
"50"
"60"
"70"
"80"
"90"
"100"
"12345678910"
"k = 8"
"7"
"6"
"5"
"4"
"3"
"2"
"1"
"0"
"0"
"Megabytes ("
"    "
"            )"
"*"
"b"
"8"
"10"
"6"
"FIGURE 2.21"
"Number of  "
"megabytes "
"required to store "
"images for  "
"various values of "
"N and k."
"DIP","4E_GLOBAL_Print_Ready.indb   706/16/2017   2:02:19 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"71"
"Conversely, the coordinate indices for a given linear index value a are given by the "
"equations"
"†"
"x","M=amod"
"(2-15)"
"and"
" "
"yx","M="
"()"
"a-"
"(2-16)"
"Recall  that  amod","M  means  “the  remainder  of  the  division  of  a  by  M.”  This  is  a  "
"formal way of stating that row numbers repeat themselves at the start of every col-"
"umn. Thus, when a=0 , the remainder of the division of 0 by M is 0, so x=0 . When "
"a=1,  the remainder is 1, and so x=1. You can see that x will continue to be equal "
"to a until a=−M","1. When a=M (which is at the beginning of the second column), "
"the remainder is 0, and thus x=0  again, and it increases by 1 until the next column "
"is reached, when the pattern repeats itself. Similar comments apply to Eq. (2-16). See "
"Problem 2.11 for a derivation of the preceding two equations."
"SPATIAL AND INTENSITY RESOLUTION"
"Intuitively, spatial  resolution  is  a  measure  of  the  smallest  discernible  detail  in  an  "
"image.  Quantitatively,  spatial  resolution  can  be  stated  in  several  ways,  with  line "
"pairs per unit distance, and dots (pixels) per unit distance being common measures. "
"Suppose  that  we  construct  a  chart  with  alternating  black  and  white  vertical  lines,  "
"each of width W units (W can be less than 1). The width of a line pair is thus 2W, and "
"there are W","2  line pairs per unit distance. For example, if the width of a line is 0.1 mm, "
"there  are  5  line  pairs  per  unit  distance  (i.e.,  per  mm).  A  widely  used  definition  of  "
"image resolution is the largest number of discernible line pairs per unit distance (e.g., "
"100 line pairs per mm). Dots per unit distance is a measure of image resolution used "
"in the printing and publishing industry. In the U.S., this measure usually is expressed "
"as dots per inch (dpi). To give you an idea of quality, newspapers are printed with a "
"†"
"When working with modular number systems, it is more accurate to write x","M≡amod, where the symbol ≡"
"means congruence. However, our interest here is just on converting from linear to coordinate indexing, so we "
"use the more familiar equal sign."
"x"
"y"
"Image f(x, y)"
"(0, 0)  α = 0"
"(M - 1, 0)  α = M - 1"
"(M - 1, N - 1)  α = MN - 1"
"(0, 1)  α = M"
"(0, 2)  α = 2M"
"(M - 1, 1)  α = 2M - 1"
"Image f(ffx, y)"
"FIGURE 2.22"
"Illustration of  "
"column scanning "
"for generating  "
"linear indices. "
"Shown are several "
"2-D coordinates (in "
"parentheses) and "
"their corresponding "
"linear indices."
"DIP","4E_GLOBAL_Print_Ready.indb   716/16/2017   2:02:20 PM"
"www.EBooks","World.ir"
""
"72"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"resolution of 75 dpi, magazines at 133 dpi, glossy brochures at 175 dpi, and the book "
"page at which you are presently looking was printed at 2400 dpi. "
"To be meaningful, measures of spatial resolution must be stated with respect to "
"spatial units. Image size by itself does not tell the complete story. For example, to say "
"that an image has a resolution of 10241024* pixels is not a meaningful statement "
"without  stating  the  spatial  dimensions  encompassed  by  the  image.  Size  by  itself  is  "
"helpful  only  in  making  comparisons  between  imaging  capabilities.  For  instance,  a  "
"digital  camera  with  a  20-megapixel  CCD  imaging  chip  can  be  expected  to  have  a  "
"higher capability to resolve detail than an 8-megapixel camera, assuming that both "
"cameras are equipped with comparable lenses and the comparison images are taken "
"at the same distance."
"Intensity  resolution  similarly  refers  to  the  smallest  discernible  change  in  inten-"
"sity level. We have considerable discretion regarding the number of spatial samples "
"(pixels) used to generate a digital image, but this is not true regarding the number "
"of intensity levels. Based on hardware considerations, the number of intensity levels "
"usually is an integer power of two, as we mentioned when discussing Eq. (2-11). The "
"most common number is 8 bits, with 16 bits being used in some applications in which "
"enhancement  of  specific  intensity  ranges  is  necessary.  Intensity  quantization  using  "
"32 bits is rare. Sometimes one finds systems that can digitize the intensity levels of "
"an image using 10 or 12 bits, but these are not as common. "
"Unlike spatial resolution, which must be based on a per-unit-of-distance basis to "
"be  meaningful,  it  is  common  practice  to  refer  to  the  number  of  bits  used  to  quan-"
"tize intensity as the “intensity resolution.” For example, it is common to say that an "
"image whose intensity is quantized into 256 levels has 8 bits of intensity resolution. "
"However, keep in mind that discernible changes in intensity are influenced also by "
"noise and saturation values, and by the capabilities of human perception to analyze "
"and interpret details in the context of an entire scene (see Section 2.1). The following "
"two examples illustrate the effects of spatial and intensity resolution on discernible "
"detail.  Later  in  this  section,  we  will  discuss  how  these  two  parameters  interact  in  "
"determining perceived image quality."
"EXAMPLE 2.2 : Effects of reducing the spatial resolution of a digital image."
"Figure 2.23 shows the effects of reducing the spatial resolution of an image. The images in Figs. 2.23(a) "
"through (d) have resolutions of 930, 300, 150, and 72 dpi, respectively. Naturally, the lower resolution "
"images are smaller than the original image in (a). For example, the original image is of size 21362140* "
"pixels, but the 72 dpi image is an array of only 165166* pixels. In order to facilitate comparisons, all the "
"smaller images were zoomed back to the original size (the method used for zooming will be discussed "
"later in this section). This is somewhat equivalent to “getting closer” to the smaller images so that we can "
"make comparable statements about visible details. "
"There are some small visual differences between Figs. 2.23(a) and (b), the most notable being a slight "
"distortion in the seconds marker pointing to 60 on the right side of the chronometer. For the most part, "
"however, Fig. 2.23(b) is quite acceptable. In fact, 300 dpi is the typical minimum image spatial resolution "
"used for book publishing, so one would not expect to see much difference between these two images. "
"Figure 2.23(c) begins to show visible degradation (see, for example, the outer edges of the chronometer "
"DIP","4E_GLOBAL_Print_Ready.indb   726/16/2017   2:02:20 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"73"
"case and compare the seconds marker with the previous two images). The numbers also show visible "
"degradation. Figure 2.23(d) shows degradation that is visible in most features of the image. When print-"
"ing at such low resolutions, the printing and publishing industry uses a number of techniques (such as "
"locally varying the pixel size) to produce much better results than those in Fig. 2.23(d). Also, as we will "
"show later in this section, it is possible to improve on the results of Fig. 2.23 by the choice of interpola-"
"tion method used."
"EXAMPLE 2.3 :  Effects of varying the number of intensity levels in a digital image."
"Figure 2.24(a) is a 774640× CT projection image, displayed using 256 intensity levels (see Chapter 1 "
"regarding CT images). The objective of this example is to reduce the number of intensities of the image "
"from  256  to  2  in  integer  powers  of  2,  while  keeping  the  spatial  resolution  constant.  Figures  2.24(b)  "
"through (d) were obtained by reducing the number of intensity levels to 128, 64, and 32, respectively (we "
"will discuss in Chapter 3 how to reduce the number of levels). "
"ba"
"dc"
"FIGURE 2.23"
"Effects of  "
"reducing spatial "
"resolution. The "
"images shown "
"are at:  "
"(a) 930 dpi,  "
"(b) 300 dpi,  "
"(c) 150 dpi, and "
"(d) 72 dpi."
"DIP","4E_GLOBAL_Print_Ready.indb   736/16/2017   2:02:21 PM"
"www.EBooks","World.ir"
""
"74"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"The 128- and 64-level images are visually identical for all practical purposes. However, the 32-level image "
"in Fig. 2.24(d) has a set of almost imperceptible, very fine ridge-like structures in areas of constant inten-"
"sity. These structures are clearly visible in the 16-level image in Fig. 2.24(e). This effect, caused by using "
"an insufficient number of intensity levels in smooth areas of a digital image, is called false contouring, so "
"named because the ridges resemble topographic contours in a map. False contouring generally is quite "
"objectionable in images displayed using 16 or fewer uniformly spaced intensity levels, as the images in "
"Figs. 2.24(e)-(h) show. "
"As a very rough guideline, and assuming integer powers of 2 for convenience, images of size 256256* "
"pixels with 64 intensity levels, and printed on a size format on the order of 55* cm, are about the lowest "
"spatial and intensity resolution images that can be expected to be reasonably free of objectionable sam-"
"pling distortions and false contouring."
"ba"
"dc"
"FIGURE 2.24"
"(a) 774 × 640, "
"256-level image. "
"(b)-(d) Image  "
"displayed in 128, "
"64, and 32 inten-"
"sity levels, while  "
"keeping the  "
"spatial resolution  "
"constant.  "
"(Original image "
"courtesy of the "
"Dr. David R.  "
"Pickens,  "
"Department of "
"Radiology & "
"Radiological  "
"Sciences,  "
"Vanderbilt  "
"University  "
"Medical Center.)"
"DIP","4E_GLOBAL_Print_Ready.indb   746/16/2017   2:02:22 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"75"
"The results in Examples 2.2 and 2.3 illustrate the effects produced on image qual-"
"ity by varying spatial and intensity resolution independently. However, these results "
"did  not  consider  any  relationships  that  might  exist  between  these  two  parameters.  "
"An early study by Huang [1965] attempted to quantify experimentally the effects on "
"image  quality  produced  by  the  interaction  of  these  two  variables.  The  experiment  "
"consisted of a set of subjective tests. Images similar to those shown in Fig. 2.25 were "
"used. The woman’s face represents an image with relatively little detail; the picture "
"of the cameraman contains an intermediate amount of detail; and the crowd picture "
"contains, by comparison, a large amount of detail. "
"Sets of these three types of images of various sizes and intensity resolution were "
"generated by varying N and k [see Eq. (2-13)]. Observers were then asked to rank "
"fe"
"h"
"g"
"FIGURE 2.24"
"(Continued) "
"(e)-(h) Image "
"displayed in 16, 8, "
"4, and 2 intensity "
"levels. "
"DIP","4E_GLOBAL_Print_Ready.indb   756/16/2017   2:02:22 PM"
"www.EBooks","World.ir"
""
"76"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"them according to their subjective quality. Results were summarized in the form of "
"so-called isopreference curves in the Nk-plane. (Figure 2.26 shows average isopref-"
"erence curves representative of the types of images in Fig. 2.25.) Each point in the "
"Nk-plane  represents  an  image  having  values  of  N  and  k  equal  to  the  coordinates  "
"of that point. Points lying on an isopreference curve correspond to images of equal "
"subjective quality. It was found in the course of the experiments that the isoprefer-"
"ence curves tended to shift right and upward, but their shapes in each of the three "
"image categories were similar to those in Fig. 2.26. These results were not unexpect-"
"ed, because a shift up and right in the curves simply means larger values for N and k, "
"which implies better picture quality."
"bac"
"FIGURE 2.25"
" (a) Image with a low level of detail. (b) Image with a medium level of detail. (c) Image with a relatively "
"large amount of detail. (Image (b) courtesy of the Massachusetts Institute of Technology.)"
"Face"
"2561286432"
"4"
"5"
"k"
"N"
"Crowd"
"Cameraman"
"FIGURE 2.26"
"Representative  "
"isopreference "
"curves for the "
"three types of  "
"images in  "
"Fig. 2.25."
"DIP","4E_GLOBAL_Print_Ready.indb   766/16/2017   2:02:22 PM"
"www.EBooks","World.ir"
""
"2.4"
"  "
"Image Sampling and Quantization"
"    "
"77"
"Observe that isopreference curves tend to become more vertical as the detail in "
"the image increases. This result suggests that for images with a large amount of detail "
"only a few intensity levels may be needed. For example, the isopreference curve in "
"Fig. 2.26 corresponding to the crowd is nearly vertical. This indicates that, for a fixed "
"value of N, the perceived quality for this type of image is nearly independent of the "
"number of intensity levels used (for the range of intensity levels shown in Fig. 2.26). "
"The perceived quality in the other two image categories remained the same in some "
"intervals in which the number of samples was increased, but the number of intensity "
"levels actually decreased. The most likely reason for this result is that a decrease in k "
"tends to increase the apparent contrast, a visual effect often perceived as improved "
"image quality."
"IMAGE INTERPOLATION"
"Interpolation is used in tasks such as zooming, shrinking, rotating, and geometrically "
"correcting digital images. Our principal objective in this section is to introduce inter-"
"polation and apply it to image resizing (shrinking and zooming), which are basically "
"image  resampling  methods.  Uses  of  interpolation  in  applications  such  as  rotation  "
"and geometric corrections will be discussed in Section 2.6."
"Interpolation is the process of using known data to estimate values at unknown "
"locations. We begin the discussion of this topic with a short example. Suppose that "
"an image of size 500500* pixels has to be enlarged 1.5 times to 750750* pixels. A "
"simple way to visualize zooming is to create an imaginary 750750* grid with the "
"same pixel spacing as the original image, then shrink it so that it exactly overlays the "
"original image. Obviously, the pixel spacing in the shrunken 750750* grid will be "
"less than the pixel spacing in the original image. To assign an intensity value to any "
"point in the overlay, we look for its closest pixel in the underlying original image and "
"assign the intensity of that pixel to the new pixel in the 750750* grid. When intensi-"
"ties have been assigned to all the points in the overlay grid, we expand it back to the "
"specified size to obtain the resized image."
"The  method  just  discussed  is  called  nearest  neighbor  interpolation  because  it  "
"assigns  to  each  new  location  the  intensity  of  its  nearest  neighbor  in  the  original  "
"image (see Section 2.5 regarding neighborhoods). This approach is simple but, it has "
"the tendency to produce undesirable artifacts, such as severe distortion of straight "
"edges. A more suitable approach is bilinear interpolation, in which we use the four "
"nearest neighbors to estimate the intensity at a given location. Let (, )","xy denote the "
"coordinates of the location to which we want to assign an intensity value (think of "
"it as a point of the grid described previously), and let v(,)","xy denote that intensity "
"value. For bilinear interpolation, the assigned value is obtained using the equation"
" v(,)","xyax   by   cxy   d=++  + "
"(2-17)"
"where  the  four  coefficients  are  determined  from  the  four  equations  in  four  "
"unknowns  that  can  be  written  using  the  four  nearest  neighbors  of  point  (, )","xy. "
"Bilinear interpolation gives much better results than nearest neighbor interpolation, "
"with a modest increase in computational burden."
"Contrary to what the "
"name suggests, bilinear "
"interpolation is not a "
"linear operation because "
"it involves multiplication "
"of coordinates (which is "
"not a linear operation). "
"See Eq. (2-17)."
"DIP","4E_GLOBAL_Print_Ready.indb   776/16/2017   2:02:23 PM"
"www.EBooks","World.ir"
""
"78"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"The next level of complexity is bicubic interpolation, which involves the sixteen "
"nearest neighbors of a point. The intensity value assigned to point (, )","xy is obtained "
"using the equation"
" v(,)","xyaxy"
"ij"
"ij"
"ji"
"="
"=="
"∑∑"
"0"
"3"
"0"
"3"
"  "
"(2-18)"
"The  sixteen  coefficients  are  determined  from  the  sixteen  equations  with  six-"
"teen  unknowns  that  can  be  written  using  the  sixteen  nearest  neighbors  of  point  "
"(, )","xy  .  Observe  that  Eq.  (2-18)  reduces  in  form  to  Eq.  (2-17)  if  the  limits  of  both  "
"summations in the former equation are 0 to 1. Generally, bicubic interpolation does "
"a better job of preserving fine detail than its bilinear counterpart. Bicubic interpola-"
"tion is the standard used in commercial image editing applications, such as Adobe "
"Photoshop and Corel Photopaint."
"Although images are displayed with integer coordinates, it is possible during pro-"
"cessing  to  work  with  subpixel  accuracy  by  increasing  the  size  of  the  image  using  "
"interpolation to “fill the gaps” between pixels in the original image."
"EXAMPLE 2.4 :  Comparison of interpolation approaches for image shrinking and zooming."
"Figure 2.27(a) is the same as Fig. 2.23(d), which was obtained by reducing the resolution of the 930 dpi "
"image in Fig. 2.23(a) to 72 dpi (the size shrank from 21362140* to 165166* pixels) and then zooming "
"the reduced image back to its original size. To generate Fig. 2.23(d) we used nearest neighbor interpola-"
"tion both to shrink and zoom the image. As noted earlier, the result in Fig. 2.27(a) is rather poor. Figures "
"2.27(b) and (c) are the results of repeating the same procedure but using, respectively, bilinear and bicu-"
"bic interpolation for both shrinking and zooming. The result obtained by using bilinear interpolation is a "
"significant improvement over nearest neighbor interpolation, but the resulting image is blurred slightly. "
"Much sharper results can be obtained using bicubic interpolation, as Fig. 2.27(c) shows. "
" "
"FIGURE 2.27 (a) Image reduced to 72 dpi and zoomed back to its original 930 dpi using nearest neighbor interpolation. "
"This figure is the same as Fig. 2.23(d). (b) Image reduced to 72 dpi and zoomed using bilinear interpolation. (c) Same "
"as (b) but using bicubic interpolation."
"bac"
"DIP","4E_GLOBAL_Print_Ready.indb   786/16/2017   2:02:24 PM"
"www.EBooks","World.ir"
""
"2.5"
"  "
"Some Basic Relationships Between Pixels"
"    "
"79"
"It is possible to use more neighbors in interpolation, and there are more complex "
"techniques, such as using splines or wavelets, that in some instances can yield better "
"results than the methods just discussed. While preserving fine detail is an exception-"
"ally important consideration in image generation for 3-D graphics (for example, see "
"Hughes  and  Andries  [2013]),  the  extra  computational  burden  seldom  is  justifiable  "
"for  general-purpose  digital  image  processing,  where  bilinear  or  bicubic  interpola-"
"tion typically are the methods of choice."
"2.5 SOME BASIC RELATIONSHIPS BETWEEN PIXELS  "
"In this section, we discuss several important relationships between pixels in a digital "
"image. When referring in the following discussion to particular pixels, we use lower-"
"case letters, such as p and q."
"NEIGHBORS OF A PIXEL"
"A  pixel  p  at  coordinates  (, )","xy  has  two  horizontal  and  two  vertical  neighbors  with  "
"coordinates"
" (,   ), (,   ), (  ,), (  ,)","xyxyxy   xy+−    +−11    11"
"This set of pixels, called the 4-neighbors of p, is denoted Np"
"4"
"()."
"The four diagonal neighbors of p have coordinates"
" (, ),(, ),(, ),(, )","xyxy   xy   xy++   +−   −+   −−11   11   11   11"
"and are denoted Np"
"D"
"(). These neighbors, together with the 4-neighbors, are called "
"the 8-neighbors of p, denoted by Np"
"8"
"(). The set of image locations of the neighbors "
"of a point p is called the neighborhood of p. The neighborhood is said to be closed if "
"it contains p. Otherwise, the neighborhood is said to be open."
"ADJACENCY, CONNECTIVITY, REGIONS, AND BOUNDARIES"
"Let V  be  the  set  of  intensity  values  used  to  define  adjacency.  In  a  binary  image,  "
"V="
"{}"
"1 if we are referring to adjacency of pixels with value 1. In a grayscale image, "
"the idea is the same, but set V typically contains more elements. For example, if we "
"are dealing with the adjacency of pixels whose values are in the range 0 to 255, set V "
"could be any subset of these 256 values. We consider three types of adjacency:"
"1. 4-adjacency. Two pixels p and q with values from V are 4-adjacent if q is in the "
"set Np"
"4"
"()."
"2. 8-adjacency. Two pixels p and q with values from V are 8-adjacent if q is in the "
"set Np"
"8"
"()."
"3. m-adjacency (also called mixed adjacency). Two pixels p and q with values from "
"V are m-adjacent if"
"2.5"
"DIP","4E_GLOBAL_Print_Ready.indb   796/16/2017   2:02:24 PM"
"www.EBooks","World.ir"
""
"80"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"(a) q is in Np"
"4"
"(), or"
"(b) q  is  in  Np"
"D"
"() and  the  set  Np  Nq"
"44"
"()() ̈  has  no  pixels  whose  values  are  "
"from V."
"Mixed adjacency is a modification of 8-adjacency, and is introduced to eliminate the "
"ambiguities that may result from using 8-adjacency. For example, consider the pixel "
"arrangement in Fig. 2.28(a) and let V="
"{}"
"1. The three pixels at the top of Fig. 2.28(b) "
"show multiple (ambiguous) 8-adjacency, as indicated by the dashed lines. This ambi-"
"guity is removed by using m-adjacency, as in Fig. 2.28(c). In other words, the center "
"and upper-right diagonal pixels are not m-adjacent because they do not satisfy con-"
"dition (b)."
"A digital path  (or  curve)  from  pixel  p  with  coordinates  (, )","xy"
"00"
"  to  pixel  q  with  "
"coordinates (, )","xy"
"nn"
" is a sequence of distinct pixels with coordinates"
" "
"(    ,), (    ,),, (,)","xyxyxy"
"nn","0011"
"..."
"where points (, )","xy"
"ii"
" and (, )","xy"
"ii−−11"
" are adjacent for 1≤≤in. In this case, n is the "
"length of the path. If (, ) (, )","xyxy"
"nn","00"
"= the path is a closed path. We can define 4-, 8-, "
"or m-paths, depending on the type of adjacency specified. For example, the paths in "
"Fig. 2.28(b) between the top right and bottom right points are 8-paths, and the path "
"in Fig. 2.28(c) is an m-path."
"Let S represent a subset of pixels in an image. Two pixels p and q are said to be "
"connected in S if there exists a path between them consisting entirely of pixels in S. "
"For any pixel p in S, the set of pixels that are connected to it in S is called a connected "
"component  of  S.  If  it  only  has  one  component,  and  that  component  is  connected,  "
"then S is called a connected set."
"Let R represent a subset of pixels in an image. We call R a region of the image if R "
"is a connected set. Two regions, R"
"i"
" and R"
"j"
" are said to be adjacent if their union forms "
"a connected set. Regions that are not adjacent are said to be disjoint. We consider 4- "
"and 8-adjacency when referring to regions. For our definition to make sense, the type "
"of adjacency used must be specified. For example, the two regions of 1’s in Fig. 2.28(d) "
"are adjacent only if 8-adjacency is used (according to the definition in the previous "
"We use the symbols "
" ̈ and  ́ to denote set "
"intersection and union, "
"respectively. Given sets "
"A and B, recall that "
"their intersection is the "
"set of elements that "
"are members of both "
"A and B. The union of "
"these two sets is the set "
"of elements that are "
"members of A, of B, or "
"of both. We will discuss "
"sets in more detail in "
"Section 2.6."
"011"
"010"
"001"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"1"
"1"
"1"
"1"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"111"
"101"
"010"
"R"
"i"
"R"
"j"
"001"
"111"
"111"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"1"
"1"
"1"
"1"
"0"
"0"
"1"
"1"
"1"
"1"
"0"
"0"
"0"
"0"
"1"
"1"
"0"
"0"
"0"
"0"
"0"
"0"
"0"
"011"
"010"
"001"
"0"
"0"
"0"
"11"
"10"
"01"
"bacedf"
"FIGURE  2.28"
"  (a)  An  arrangement  of  pixels.  (b)  Pixels  that  are  8-adjacent  (adjacency  is  shown  by  dashed  lines).   "
"(c) m-adjacency. (d) Two regions (of 1’s) that are 8-adjacent. (e) The circled point is on the boundary of the 1-valued "
"pixels only if 8-adjacency between the region and background is used. (f) The inner boundary of the 1-valued region "
"does not form a closed path, but its outer boundary does."
"DIP","4E_GLOBAL_Print_Ready.indb   806/16/2017   2:02:25 PM"
"www.EBooks","World.ir"
""
"2.5"
"  "
"Some Basic Relationships Between Pixels"
"    "
"81"
"paragraph, a 4-path between the two regions does not exist, so their union is not a "
"connected set)."
"Suppose an image contains K disjoint regions, Rk","K"
"k"
",,,,,=12... none of which "
"touches  the  image  border."
"†"
"  Let  R"
"u"
"  denote  the  union  of  all  the  K  regions,  and  let  "
"R"
"u"
"c"
"()"
"  denote  its  complement  (recall  that  the  complement  of  a  set  A  is  the  set  of  "
"points  that  are  not  in  A).  We  call  all  the  points  in  R"
"u"
"  the  foreground,  and  all  the  "
"points in R"
"u"
"c"
"()"
" the background of the image."
"The boundary (also called the border or contour) of a region R is the set of pixels in "
"R that are adjacent to pixels in the complement of R. Stated another way, the border "
"of a region is the set of pixels in the region that have at least one background neigh-"
"bor. Here again, we must specify the connectivity being used to define adjacency. For "
"example, the point circled in Fig. 2.28(e) is not a member of the border of the 1-val-"
"ued region if 4-connectivity is used between the region and its background, because "
"the  only  possible  connection  between  that  point  and  the  background  is  diagonal.  "
"As a rule, adjacency between points in a region and its background is defined using "
"8-connectivity to handle situations such as this."
"The  preceding  definition  sometimes  is  referred  to  as  the  inner  border  of  the  "
"region to distinguish it from its outer border, which is the corresponding border in "
"the background. This distinction is important in the development of border-follow-"
"ing algorithms. Such algorithms usually are formulated to follow the outer boundary "
"in order to guarantee that the result will form a closed path. For instance, the inner "
"border of the 1-valued region in Fig. 2.28(f) is the region itself. This border does not "
"satisfy  the  definition  of  a  closed  path.  On  the  other  hand,  the  outer  border  of  the  "
"region does form a closed path around the region."
"If R happens to be an entire image, then its boundary (or border) is defined as the "
"set of pixels in the first and last rows and columns of the image. This extra definition "
"is required because an image has no neighbors beyond its border. Normally, when "
"we refer to a region, we are referring to a subset of an image, and any pixels in the "
"boundary  of  the  region  that  happen  to  coincide  with  the  border  of  the  image  are  "
"included implicitly as part of the region boundary."
"The  concept  of  an  edge  is  found  frequently  in  discussions  dealing  with  regions  "
"and boundaries. However, there is a key difference between these two concepts. The "
"boundary of a finite region forms a closed path and is thus a “global” concept. As we "
"will discuss in detail in Chapter 10, edges are formed from pixels with derivative val-"
"ues that exceed a preset threshold. Thus, an edge is a “local” concept that is based on "
"a measure of intensity-level discontinuity at a point. It is possible to link edge points "
"into  edge  segments,  and  sometimes  these  segments  are  linked  in  such  a  way  that  "
"they correspond to boundaries, but this is not always the case. The one exception in "
"which edges and boundaries correspond is in binary images. Depending on the type "
"of  connectivity  and  edge  operators  used  (we  will  discuss  these  in  Chapter  10),  the  "
"edge extracted from a binary region will be the same as the region boundary. This is "
"†"
"  We make this assumption to avoid having to deal with special cases. This can be done without loss of generality "
"because if one or more regions touch the border of an image, we can simply pad the image with a 1-pixel-wide "
"border of background values."
"DIP","4E_GLOBAL_Print_Ready.indb   816/16/2017   2:02:26 PM"
"www.EBooks","World.ir"
""
"82"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"intuitive. Conceptually, until we arrive at Chapter 10, it is helpful to think of edges "
"as intensity discontinuities, and of boundaries as closed paths."
"DISTANCE MEASURES"
"For pixels p, q, and s, with coordinates (, )","xy, (, )","uv, and (,),wz respectively, D"
"is a distance function or metric if"
"(a) Dpq","D pqp   q(,)( (,))≥00==iff,"
"(b) Dpq","Dqp(,)(, )=, and"
"(c) Dps","Dpq    Dqs(,)(, )(,).≤+"
"The Euclidean distance between p and q is defined as"
" "
"Dpqxy"
"e"
"(, )   ()   ()=−+−"
"⎡"
"⎣"
"⎤"
"⎦"
"uv"
"22"
"1"
"2"
" "
"(2-19)"
"For  this  distance  measure,  the  pixels  having  a  distance  less  than  or  equal  to  some  "
"value r from (, )","xy are the points contained in a disk of radius r centered at (, )","xy."
"The D"
"4"
" distance, (called the city-block distance) between p and q is defined as"
" Dpq    xy"
"4"
"(, )=−−uv+ "
"(2-20)"
"In this case, pixels having a D"
"4"
" distance from (, )","xy that is less than or equal to some "
"value d form a diamond centered at (, )","xy. For example, the pixels with D"
"4"
" distance ≤2 "
"from (, )","xy (the center point) form the following contours of constant distance:"
" "
"2"
"212"
"21012"
"212"
"2"
"The pixels with D"
"4"
"1= are the 4-neighbors of (, )","xy."
"The D"
"8"
" distance (called the chessboard distance) between p and q is defined as"
" Dpqxy"
"8"
"(   ,   )","max(,)=−−uv "
"(2-21)"
"In this case, the pixels with "
"D"
"8"
" distance from (, )","xy less than or equal to some value d "
"form a square centered at (, )","xy. For example, the pixels with D"
"8"
" distance ≤2 form "
"the following contours of constant distance:"
" "
"22222"
"21112"
"21012"
"21112"
"22222"
"The pixels with "
"D"
"8"
"1= are the 8-neighbors of the pixel at (, )","xy."
"DIP","4E_GLOBAL_Print_Ready.indb   826/16/2017   2:02:28 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"83"
"Note that the D"
"4"
" and D"
"8"
" distances between p and q are independent of any paths "
"that might exist between these points because these distances involve only the coor-"
"dinates of the points. In the case of m-adjacency, however, the D"
"m"
" distance between "
"two  points  is  defined  as  the  shortest  m-path  between  the  points.  In  this  case,  the  "
"distance between two pixels will depend on the values of the pixels along the path, "
"as well as the values of their neighbors. For instance, consider the following arrange-"
"ment of pixels and assume that p, p"
"2"
", and p"
"4"
" have a value of 1, and that p"
"1"
" and p"
"3"
" "
"can be 0 or 1:"
" "
"pp"
"pp"
"p"
"34"
"12"
"Suppose that we consider adjacency of pixels valued 1 (i.e.,V="
"{}"
"1). If p"
"1"
" and p"
"3"
" are 0, "
"the length of the shortest m-path (the D"
"m"
" distance) between p and p"
"4"
" is 2. If p"
"1"
" is 1, "
"then p"
"2"
" and p will no longer be m-adjacent (see the definition of m-adjacency given "
"earlier) and the length of the shortest m-path becomes 3 (the path goes through the "
"points pp p p"
"124"
").  Similar  comments  apply  if  p"
"3"
"  is  1  (and  p"
"1"
"  is  0);  in  this  case,  the  "
"length of the shortest m-path also is 3. Finally, if both p"
"1"
" and p"
"3"
" are 1, the length of "
"the shortest m-path between p and p"
"4"
" is 4. In this case, the path goes through the "
"sequence of points pp p p p"
"1234"
"."
"2.6   INTRODUCTION TO THE BASIC MATHEMATICAL TOOLS USED IN "
"DIGITAL IMAGE PROCESSING "
"This  section  has  two  principal  objectives:  (1)  to  introduce  various  mathematical  "
"tools we use throughout the book; and (2) to help you begin developing a “feel” for "
"how  these  tools  are  used  by  applying  them  to  a  variety  of  basic  image-processing  "
"tasks, some of which will be used numerous times in subsequent discussions. "
"ELEMENTWISE VERSUS MATRIX OPERATIONS"
"An elementwise operation involving one or more images is carried out on a pixel-by-"
"pixel basis. We mentioned earlier in this chapter that images can be viewed equiva-"
"lently  as  matrices.  In  fact,  as  you  will  see  later  in  this  section,  there  are  many  situ-"
"ations  in  which  operations  between  images  are  carried  out  using  matrix  theory.  It  "
"is  for  this  reason  that  a  clear  distinction  must  be  made  between  elementwise  and  "
"matrix operations. For example, consider the following 22* images (matrices):"
" "
"aa"
"aa"
"bb"
"bb"
"1112"
"2122"
"1112"
"2122"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"and"
"The elementwise  product  (often  denoted  using  the  symbol  }  or  z)  of  these  two  "
"images is"
" "
"aa"
"aa"
"bb"
"bb"
"abab"
"ab    a"
"1112"
"2122"
"1112"
"2122"
"11   1112   12"
"21   212"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"=}"
"2222"
"b"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"2.6"
"You may find it helpful "
"to download and study "
"the review material "
"dealing with probability, "
"vectors, linear algebra, "
"and linear systems. The "
"review is available in the "
"Tutorials section of the "
"book website. "
"The elementwise product "
"of two matrices is also "
"called the Hadamard "
"product of the matrices."
"The symbol | is often "
"used to denote element-"
"wise division."
"DIP","4E_GLOBAL_Print_Ready.indb   836/16/2017   2:02:29 PM"
"www.EBooks","World.ir"
""
"84"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"That is, the elementwise product is obtained by multiplying pairs of corresponding "
"pixels. On the other hand, the matrix product of the images is formed using the rules "
"of matrix multiplication:"
" "
"aa"
"aa"
"bb"
"bb"
"abababa"
"1112"
"2122"
"1112"
"2122"
"11   1112   2111   12"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"="
"++"
"112   22"
"21   1122   2121   1222   22"
"b"
"abab    abab++"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"We assume elementwise operations throughout the book, unless stated otherwise. "
"For example, when we refer to raising an image to a power, we mean that each indi-"
"vidual pixel is raised to that power; when we refer to dividing an image by another, "
"we mean that the division is between corresponding pixel pairs, and so on. The terms "
"elementwise addition and subtraction of two images are redundant because these are "
"elementwise operations by definition. However, you may see them used sometimes "
"to clarify notational ambiguities. "
"LINEAR VERSUS NONLINEAR OPERATIONS"
"One of the most important classifications of an image processing method is whether "
"it  is  linear  or  nonlinear.  Consider  a  general  operator,  ,   that  produces  an  output  "
"image, gxy(,), from a given input image, fxy(,):"
" fxygxy(, )(, )"
"[]"
"= "
"(2-22)"
"Given two arbitrary constants, a and b, and two arbitrary images fxy"
"1"
"(, ) and fxy"
"2"
"(, ),"
" is said to be a linear operator if"
" "
"af  xybf  xyaf  xybf  xy"
"ag   x ybg"
"1212"
"12"
"(, )(, )(, )(, )"
"(, )("
"+"
"[]"
"="
"[]"
"+"
"[]"
"=+xxy,)"
" "
"(2-23)"
"This equation indicates that the output of a linear operation applied to the sum of "
"two inputs is the same as performing the operation individually on the inputs and "
"then  summing  the  results.  In  addition,  the  output  of  a  linear  operation  on  a  con-"
"stant multiplied by an input is the same as the output of the operation due to the "
"original input multiplied by that constant. The first property is called the property "
"of additivity, and the second is called the property of homogeneity. By definition, an "
"operator that fails to satisfy Eq. (2-23) is said to be nonlinear."
"As an example, suppose that  is the sum operator, Σ. The function performed "
"by this operator is simply to sum its inputs. To test for linearity, we start with the left "
"side of Eq. (2-23) and attempt to prove that it is equal to the right side:"
" "
"af  xybf  xyaf  xybf  xy"
"af  xyb    f  xy"
"1212"
"12"
"(, )(, )(, )(, )"
"(, )(, )"
"+"
"[]"
"=+"
"=+"
"∑∑∑"
"∑∑∑"
"=+ag  xybg  xy"
"12"
"(, )(, )"
" "
"where  the  first  step  follows  from  the  fact  that  summation  is  distributive.  So,  an  "
"expansion of the left side is equal to the right side of Eq. (2-23), and we conclude "
"that the sum operator is linear."
"These are image  "
"summations, not the "
"sums of all the elements "
"of an image. "
"DIP","4E_GLOBAL_Print_Ready.indb   846/16/2017   2:02:30 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"85"
"On the other hand, suppose that we are working with the max operation, whose "
"function  is  to  find  the  maximum  value  of  the  pixels  in  an  image.  For  our  purposes  "
"here, the simplest way to prove that this operator is nonlinear is to find an example "
"that fails the test in Eq. (2-23). Consider the following two images"
" ff"
"12"
"02"
"23"
"65"
"47"
"="
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"and"
"and suppose that we let a=1 and b=−1. To test for linearity, we again start with the "
"left side of Eq."
" (2-23):"
" "
"max  (  )()","max","1"
"02"
"23"
"1"
"65"
"47"
"63"
"24"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"+−"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎧"
"⎨"
"⎩"
"⎫"
"⎬"
"⎭"
"="
"−−"
"−−"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎧"
"⎨"
"⎩"
"⎫⎫"
"⎬"
"⎭"
"=−2"
"Working next with the right side, we obtain"
" ()","max(  )","max(  )1"
"02"
"23"
"1"
"65"
"47"
"3174"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎧"
"⎨"
"⎩"
"⎫"
"⎬"
"⎭"
"+−"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎧"
"⎨"
"⎩"
"⎫"
"⎬"
"⎭"
"=+"
"−=−"
"The left and right sides of Eq. (2-23) are not equal in this case, so we have proved "
"that the max operator is nonlinear."
"As you will see in the next three chapters, linear operations are exceptionally impor-"
"tant because they encompass a large body of theoretical and practical results that are "
"applicable  to  image  processing.  The  scope  of  nonlinear  operations  is  considerably  "
"more limited. However, you will encounter in the following chapters several nonlin-"
"ear image processing operations whose performance far exceeds what is achievable "
"by their linear counterparts."
"ARITHMETIC OPERATIONS"
"Arithmetic operations between two images fxy(,) and gxy(,) are denoted as"
" "
"sxyf xygxy"
"dxyf xygxy"
"pxyf xygx"
"(, )(, )(, )"
"(, )(, )(, )"
"(, )(, )(,"
"=+"
"=−"
"=×yy"
"xyf xygxy"
")"
"(, )(, )(, )","v=÷"
" "
"(2-24)"
"These  are  elementwise  operations  which,  as  noted  earlier  in  this  section,  means  "
"that   they   are   performed   between   corresponding   pixel   pairs   in   f   and   g   for   "
"x","M=−0121,, ,  ,...  and  y","N=−0121,, ,  ,.... As usual, M  and  N  are  the  row  and  "
"column  sizes  of  the  images.  Clearly,  s, d, p,  and  v  are  images  of  size  MN×  also.  "
"Note that image arithmetic in the manner just defined involves images of the same "
"size. The following examples illustrate the important role of arithmetic operations "
"in digital image processing."
"DIP","4E_GLOBAL_Print_Ready.indb   856/16/2017   2:02:31 PM"
"www.EBooks","World.ir"
""
"86"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"EXAMPLE 2.5 :  Using image addition (averaging) for noise reduction."
"Suppose that gxy(,) is a corrupted image formed by the addition of noise, h(, )","xy, to a noiseless image "
"fxy(,)"
" "
"; that is,"
" gxyf xyxy(,)(, )(, )=+h "
"(2-25)"
"where  the  assumption  is  that  at  every  pair  of  coordinates  (, )","xy  the  noise  is  uncorrelated"
"†"
"  and  has  "
"zero average value. We assume also that the noise and image values are uncorrelated (this is a typical "
"assumption for additive noise). The objective of the following procedure is to reduce the noise content "
"of the output image by adding a set of noisy input images, gxy"
"i"
"(, )."
"{}"
" This is a technique used frequently "
"for image enhancement."
"If the noise satisfies the constraints just stated, it can be shown (Problem 2.26) that if an image gxy(, ) "
"is formed by averaging K different noisy images,"
" gxy"
"K"
"gxy"
"i"
"i"
"K"
"(, )(, )="
"="
"∑"
"1"
"1"
" "
"(2-26)"
"then it follows that "
" Egxyfxy(, )(, )"
"{}"
"= "
"(2-27)"
"and"
" ss"
"hgxyxy"
"K"
"(,)(,)"
"22"
"1"
"= "
"(2-28)"
"where Egxy(, )"
"{}"
" is the expected value of gxy(, ), and s"
"gxy(,)"
"2"
" and s"
"h(,)","xy"
"2"
" are the variances of gxy(, ) and "
"h(,)","xy,  respectively,  all  at  coordinates  (, )","xy.  These  variances  are  arrays  of  the  same  size  as  the  input  "
"image, and there is a scalar variance value for each pixel location. "
"The standard deviation (square root of the variance) at any point (, )","xy in the average image is"
" ss"
"hgxyxy"
"K"
"(,)(,)"
"="
"1"
" "
"(2-29)"
"As K increases, Eqs. (2-28) and (2-29) indicate that the variability (as measured by the variance or the "
"standard deviation) of the pixel values at each location (, )","xy decreases. Because Egxyfxy(, )(, ),"
"{}"
"= "
"this means that gxy(, ) approaches the noiseless image fxy(,) as the number of noisy images used in the "
"averaging process increases. In order to avoid blurring and other artifacts in the output (average) image, "
"it is necessary that the images gxy"
"i"
"(, ) be registered (i.e., spatially aligned)."
"An  important  application  of  image  averaging  is  in  the  field  of  astronomy,  where  imaging  under  "
"very low light levels often cause sensor noise to render individual images virtually useless for analysis "
"(lowering the temperature of the sensor helps reduce noise). Figure 2.29(a) shows an 8-bit image of the "
"Galaxy Pair NGC 3314, in which noise corruption was simulated by adding to it Gaussian noise with "
"zero mean and a standard deviation of 64 intensity levels. This image, which is representative of noisy "
"astronomical  images  taken  under  low  light  conditions,  is  useless  for  all  practical  purposes.  Figures  "
"2.29(b) through (f) show the results of averaging 5, 10, 20, 50, and 100 images, respectively. We see from "
"Fig. 2.29(b) that an average of only 10 images resulted in some visible improvement. According to Eq. "
"†"
" The variance of a random variable z with mean z  is defined as Ez z{()  }−"
"2"
", where E{}"
""
" is the expected value of the argument. The covari-"
"ance of two random variables z"
"i"
" and z"
"j"
" is defined as Ez  z z  z"
"iij j"
"{()()}.−− If the variables are uncorrelated, their covariance is 0, and vice "
"versa. (Do not confuse correlation and statistical independence. If two random variables are statistically independent, their correlation is "
"zero. However, the converse is not true in general.)"
"DIP","4E_GLOBAL_Print_Ready.indb   866/16/2017   2:02:33 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"87"
"(2-29), the standard deviation of the noise in Fig. 2.29(b) is less than half (.)15045= the standard "
"deviation  of  the  noise  in  Fig.  2.29(a),  or  (.  )(  )045  6429≈  intensity  levels.  Similarly,  the  standard  devia-"
"tions of the noise in Figs. 2.29(c) through (f) are 0.32, 0.22, 0.14, and 0.10 of the original, which translates "
"approximately into 20, 14, 9, and 6 intensity levels, respectively. We see in these images a progression "
"of more visible detail as the standard deviation of the noise decreases. The last two images are visually "
"identical for all practical purposes. This is not unexpected, as the difference between the standard devia-"
"tions of their noise level is only about 3 intensity levels According to the discussion in connection with "
"Fig. 2.5, this difference is below what a human generally is able to detect."
"EXAMPLE 2.6 :  Comparing images using subtraction."
"Image subtraction is used routinely for enhancing differences between images. For example, the image "
"in  Fig.  2.30(b)  was  obtained  by  setting  to  zero  the  least-significant  bit  of  every  pixel  in  Fig.  2.30(a).  "
"Visually, these images are indistinguishable. However, as Fig. 2.30(c) shows, subtracting one image from "
"bac"
"edf"
"FIGURE  2.29"
" (a) Image of Galaxy Pair NGC 3314 corrupted by additive Gaussian noise. (b)-(f) Result of averaging "
"5, 10, 20, 50, and 1,00 noisy images, respectively. All images are of size "
"566598×"
" pixels, and all were scaled so that "
"their intensities would span the full [0, 255] intensity scale. (Original image courtesy of NASA.)"
"DIP","4E_GLOBAL_Print_Ready.indb   876/16/2017   2:02:34 PM"
"www.EBooks","World.ir"
""
"88"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"the  other  clearly  shows  their  differences.  Black  (0)  values  in  the  difference  image  indicate  locations  "
"where there is no difference between the images in Figs. 2.30(a) and (b). "
"We  saw  in  Fig.  2.23  that  detail  was  lost  as  the  resolution  was  reduced  in  the  chronometer  image  "
"shown in Fig. 2.23(a). A vivid indication of image change as a function of resolution can be obtained "
"by displaying the differences between the original image and its various lower-resolution counterparts. "
"Figure  2.31(a)  shows  the  difference  between  the  930  dpi  and  72  dpi  images.  As  you  can  see,  the  dif-"
"ferences are quite noticeable. The intensity at any point in the difference image is proportional to the "
"magnitude of the numerical difference between the two images at that point. Therefore, we can analyze "
"which areas of the original image are affected the most when resolution is reduced. The next two images "
"in Fig. 2.31 show proportionally less overall intensities, indicating smaller differences between the 930 dpi "
"image and 150 dpi and 300 dpi images, as expected. "
"bac"
"FIGURE  2.30"
"  (a)  Infrared  image  of  the  Washington,  D.C.  area.  (b)  Image  resulting  from  setting  to  zero  the  least  "
"significant bit of every pixel in (a). (c) Difference of the two images, scaled to the range [0, 255] for clarity. (Original "
"image courtesy of NASA.)"
"bac"
"FIGURE 2.31"
" (a) Difference between the 930 dpi and 72 dpi images in Fig. 2.23. (b) Difference between the 930 dpi and "
"150 dpi images. (c) Difference between the 930 dpi and 300 dpi images."
"DIP","4E_GLOBAL_Print_Ready.indb   886/16/2017   2:02:35 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"89"
"As a final illustration, we discuss briefly an area of medical imaging called mask mode radiography, a "
"commercially successful and highly beneficial use of image subtraction. Consider image differences of "
"the form"
" gxyf xyhxy(,)(, )(, )=− "
"(2-30)"
"In this case hxy(,), the mask, is an X-ray image of a region of a patient’s body captured by an intensified "
"TV camera (instead of traditional X-ray film) located opposite an X-ray source. The procedure consists "
"of injecting an X-ray contrast medium into the patient’s bloodstream, taking a series of images called "
"live images [samples of which are denoted as fxy(,)] of the same anatomical region as hxy(,), and sub-"
"tracting the mask from the series of incoming live images after injection of the contrast medium. The net "
"effect of subtracting the mask from each sample live image is that the areas that are different between "
"fxy(,) and hxy(,) appear in the output image, gxy(,), as enhanced detail. Because images can be cap-"
"tured at TV rates, this procedure outputs a video showing how the contrast medium propagates through "
"the various arteries in the area being observed."
"Figure 2.32(a) shows a mask X-ray image of the top of a patient’s head prior to injection of an iodine "
"medium into the bloodstream, and Fig. 2.32(b) is a sample of a live image taken after the medium was "
"ba"
"dc"
"FIGURE 2.32"
"  "
"Digital  "
"subtraction  "
"angiography.  "
"(a) Mask image. "
"(b) A live image. "
"(c) Difference "
"between (a) and "
"(b). (d) Enhanced "
"difference image. "
"(Figures (a) and "
"(b) courtesy of "
"the Image  "
"Sciences  "
"Institute,  "
"University "
"Medical Center, "
"Utrecht, The "
"Netherlands.)"
"DIP","4E_GLOBAL_Print_Ready.indb   896/16/2017   2:02:35 PM"
"www.EBooks","World.ir"
""
"90"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"injected. Figure 2.32(c) is the difference between (a) and (b). Some fine blood vessel structures are vis-"
"ible in this image. The difference is clear in Fig. 2.32(d), which was obtained by sharpening the image and "
"enhancing its contrast (we will discuss these techniques in the next chapter). Figure 2.32(d) is a “snap-"
"shot” of how the medium is propagating through the blood vessels in the subject’s brain."
"EXAMPLE 2.7 :  Using image multiplication and division for shading correction and for masking."
"An important application of image multiplication (and division) is shading correction. Suppose that an "
"imaging sensor produces images that can be modeled as the product of a “perfect image,” denoted by "
"fxy(,),  times  a  shading  function,  hxy(,);  that  is,  gxyf xyhxy(,)(, )(, )=.  If  hxy(,)  is  known  or  can  be  "
"estimated, we can obtain fxy(,) (or an estimate of it) by multiplying the sensed image by the inverse of "
"hxy(,) (i.e., dividing g by h using elementwise division). If access to the imaging system is possible, we "
"can obtain a good approximation to the shading function by imaging a target of constant intensity. When "
"the sensor is not available, we often can estimate the shading pattern directly from a shaded image using "
"the approaches discussed in Sections 3.5 and 9.8. Figure 2.33 shows an example of shading correction "
"using  an  estimate  of  the  shading  pattern.  The  corrected  image  is  not  perfect  because  of  errors  in  the  "
"shading pattern (this is typical), but the result definitely is an improvement over the shaded image in Fig. "
"2.33 (a). See Section 3.5 for a discussion of how we estimated Fig. 2.33 (b). Another use of image mul-"
"tiplication is in masking, also called region of interest (ROI), operations. As Fig. 2.34 shows, the process "
"consists of multiplying a given image by a mask image that has 1’s in the ROI and 0’s elsewhere. There "
"can be more than one ROI in the mask image, and the shape of the ROI can be arbitrary."
"A  few  comments  about  implementing  image  arithmetic  operations  are  in  order  "
"before we leave this section. In practice, most images are displayed using 8 bits (even "
"24-bit color images consist of three separate 8-bit channels). Thus, we expect image "
"values to be in the range from 0 to 255. When images are saved in a standard image "
"format, such as TIFF or JPEG, conversion to this range is automatic. When image "
"values exceed the allowed range, clipping or scaling becomes necessary. For example, "
"the values in the difference of two 8-bit images can range from a minimum of −255 "
"bac"
"FIGURE  2.33"
"  Shading  correction.  (a)  Shaded  test  pattern.  (b)  Estimated  shading  pattern.  (c)  Product  of  (a)  by  the  "
"reciprocal of (b). (See Section 3.5 for a discussion of how (b) was estimated.)"
"DIP","4E_GLOBAL_Print_Ready.indb   906/16/2017   2:02:37 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"91"
"to a maximum of 255, and the values of the sum of two such images can range from 0 "
"to 510. When converting images to eight bits, many software applications simply set "
"all negative values to 0 and set to 255 all values that exceed this limit. Given a digital "
"image g resulting from one or more arithmetic (or other) operations, an approach "
"guaranteeing that the full range of a values is “captured” into a fixed number of bits "
"is as follows. First, we perform the operation"
" ggg"
"m"
"=−min(  ) "
"(2-31)"
"which creates an image whose minimum value is 0. Then, we perform the operation"
" g","Kgg"
"sm  m"
"="
"[]"
"max() "
"(2-32)"
"which creates a scaled image, g"
"s"
", whose values are in the range [0, K]. When working "
"with 8-bit images, setting K=255 gives us a scaled image whose intensities span the "
"full 8-bit scale from 0 to 255.  Similar comments apply to 16-bit images or higher. This "
"approach can be used for all arithmetic operations. When performing division, we "
"have the extra requirement that a small number should be added to the pixels of the "
"divisor image to avoid division by 0."
"SET AND LOGICAL OPERATIONS"
"In this section, we discuss the basics of set theory. We also introduce and illustrate "
"some important set and logical operations."
"Basic Set Operations"
"A set is a collection of distinct objects. If a is an element of set A, then we write"
" a","A∈  "
"(2-33)"
"Similarly, if a is not an element of A we write"
" a","Ax "
"(2-34)"
"The set with no elements is called the null or empty set, and is denoted by ∅."
"These are elementwise "
"subtraction and division."
"bac"
"FIGURE 2.34"
" (a) Digital dental X-ray image. (b) ROI mask for isolating teeth with fillings (white corresponds to 1 and "
"black corresponds to 0). (c) Product of (a) and (b)."
"DIP","4E_GLOBAL_Print_Ready.indb   916/16/2017   2:02:37 PM"
"www.EBooks","World.ir"
""
"92"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"A set is denoted by the contents of two braces: {}.i For example, the expression"
" Ccc dd","D=="
"{}"
"-H,"
"means that C is the set of elements, c, such that c is formed by multiplying each of "
"the elements of set D by −1. "
"If every element of a set A is also an element of a set B, then A is said to be a "
"subset of B, denoted as"
" AB","8 "
"(2-35)"
"The union of two sets A and B, denoted as"
" CAB= ́ "
"(2-36)"
"is a set C consisting of elements belonging either to A, to B, or to both. Similarly, the "
"intersection of two sets A and B, denoted by"
" DAB= ̈ "
"(2-37)"
"is a set D consisting of elements belonging to both A and B. Sets A and B are said to "
"be disjoint or mutually exclusive if they have no elements in common, in which case,"
" AB ̈=∅ "
"(2-38)"
"The sample  space, Æ,  (also  called  the  set universe)  is  the  set  of  all  possible  set  "
"elements  in  a  given  application.  By  definition,  these  set  elements  are  members  of  "
"the sample space for that application. For example, if you are working with the set "
"of  real  numbers,  then  the  sample  space  is  the  real  line,  which  contains  all  the  real  "
"numbers. In image processing, we typically define Æ to be the rectangle containing "
"all the pixels in an image."
"The complement of a set A is the set of elements that are not in A:"
" AA"
"c"
"="
"{}"
"wwx "
"(2-39)"
"The difference of two sets A and B, denoted AB−, is defined as"
" ABA   B AB"
"c"
"−="
"{}"
"=ww   w","Hx ̈, "
"(2-40)"
"This is the set of elements that belong to A, but not to B. We can define A"
"c"
" in terms "
"of Æ and the set difference operation; that is, AA"
"c"
"=−Æ. Table 2.1 shows several "
"important set properties and relationships."
"Figure 2.35 shows diagrammatically (in so-called Venn diagrams) some of the set "
"relationships in Table 2.1. The shaded areas in the various figures correspond to the "
"set operation indicated above or below the figure. Figure 2.35(a) shows the sample "
"set, Æ. As no earlier, this is the set of all possible elements in a given application. Fig-"
"ure 2.35(b) shows that the complement of a set A is the set of all elements in Æ that "
"are not in A, which agrees with our earlier definition. Observe that Figs. 2.35(e) and "
"(g) are identical, which proves the validity of Eq. (2-40) using Venn diagrams. This "
"DIP","4E_GLOBAL_Print_Ready.indb   926/16/2017   2:02:38 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"93"
"is an example of the usefulness of Venn diagrams for proving equivalences between "
"set relationships. "
"When applying the concepts just discussed to image processing, we let sets repre-"
"sent objects (regions) in a binary image, and the elements of the sets are the (, )","xy "
"coordinates of those objects. For example, if we want to know whether two objects, "
"A and B, of a binary image overlap, all we have to do is compute AB ̈. If the result "
"is not the empty set, we know that some of the elements of the two objects overlap. "
"Keep in mind that the only way that the operations illustrated in Fig. 2.35 can make "
"sense  in  the  context  of  image  processing  is  if  the  images  containing  the  sets  are  "
"binary, in which case we can talk about set membership based on coordinates, the "
"assumption being that all members of the sets have the same intensity value (typi-"
"cally denoted by 1). We will discuss set operations involving binary images in more "
"detail in the following section and in Chapter 9."
"The  preceding  concepts  are  not  applicable  when  dealing  with  grayscale  images,  "
"because we have not defined yet a mechanism for assigning intensity values to the "
"pixels resulting from a set operation. In Sections 3.8 and 9.6 we will define the union "
"and intersection operations for grayscale values as the maximum and minimum of "
"corresponding  pixel  pairs,  respectively.  We  define  the  complement  of  a  grayscale  "
"image as the pairwise differences between a constant and the intensity of every pixel "
"in the image. The fact that we deal with corresponding pixel pairs tells us that gray-"
"scale  set  operations  are  elementwise  operations,  as  defined  earlier.  The  following  "
"example is a brief illustration of set operations involving grayscale images. We will "
"discuss these concepts further in the two sections just mentioned."
"Description","Expressions"
"Operations between the "
"sample space and null sets"
"ÆÆÆ ́ÆÆ ̈"
"cc"
"=∅  ∅ =∅=∅=∅;;;"
"Union and intersection with "
"the null and sample space sets"
"AAAAAA ́ ̈ ́ÆÆ ̈Æ∅=∅=∅==;;;"
"Union and intersection of a "
"set with itself"
"AA AAA A ́ ̈==;"
"Union and intersection of a "
"set with its complement"
"AAAA"
"cc"
" ́Æ ̈==∅;"
"Commutative laws"
"ABBA"
"ABBA"
" ́ ́"
" ̈ ̈"
"="
"="
"Associative laws"
"()()"
"()()"
"AB C A BC"
"AB C A BC"
" ́ ́ ́ ́"
" ̈ ̈ ̈ ̈"
"="
"="
"Distributive laws"
"() ()()"
"()()()"
"AB C  AC   BC"
"AB C  AC   BC"
" ́ ̈ ̈ ́ ̈"
" ̈ ́ ́ ̈ ́"
"="
"="
"De","Morgan’s laws"
" "
"()"
"()"
"AB"
"A B"
"AB   A B"
"ccc"
"ccc"
" ́ ̈"
" ̈ ́"
"="
"="
"TABLE 2.1"
"Some important "
"set operations "
"and relationships."
"DIP","4E_GLOBAL_Print_Ready.indb   936/16/2017   2:02:39 PM"
"www.EBooks","World.ir"
""
"94"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"EXAMPLE 2.8 :  Illustration of set operations involving grayscale images."
"Let the elements of a grayscale image be represented by a set A whose elements are triplets of the form "
"(, , )","xyz, where x and y are spatial coordinates, and z denotes intensity values. We define the complement "
"of A as the set "
" Axy","Kzxyz","A"
"c"
"=−"
"{}"
"(, ,)(, , )","H "
"which is the set of pixels of A whose intensities have been subtracted from a constant K. This constant "
"is  equal  to  the  maximum  intensity  value  in  the  image,  21"
"k"
"−,  where  k  is  the  number  of  bits  used  to  "
"represent z. Let A denote the 8-bit grayscale image in Fig. 2.36(a), and suppose that we want to form "
"the negative of A using grayscale set operations. The negative is the set complement, and this is an 8-bit "
"image, so all we have to do is let K=255 in the set defined above:"
" Axy  zxyz","A"
"c"
"=−"
"{}"
"(, ,)(, , )255H"
"Figure 2.36(b) shows the result. We show this only for illustrative purposes. Image negatives generally "
"are computed using an intensity transformation function, as discussed later in this section."
"A"
"c"
"AB ̈"
"AA"
"B"
"AB−"
"B"
"c"
"B"
"C"
"A"
"AB"
"c"
" ̈"
"ABC ̈ ́()"
" ́"
"AB"
"Ω"
"B"
"ba"
"d"
"c"
"f"
"h"
"e"
"g"
"FIGURE  2.35"
" Venn diagrams corresponding to some of the set operations in Table 2.1. The results of the operations, "
"such as A"
"c"
",  are shown shaded. Figures (e) and (g) are the same, proving via Venn diagrams that AB AB"
"c"
"−= ̈"
"[see Eq. (2-40)]."
"DIP","4E_GLOBAL_Print_Ready.indb   946/16/2017   2:02:40 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"95"
"The union of two grayscale sets A and B with the same number of elements is defined as the set"
" ABab a","Ab","B"
"z"
" ́HH="
"{}"
"max(  ,   ),"
"where it is understood that the max operation is applied to pairs of corresponding elements. If A and B "
"are grayscale images of the same size, we see that their the union is an array formed from the maximum "
"intensity  between  pairs  of  spatially  corresponding  elements.  As  an  illustration,  suppose  that  A  again  "
"represents the image in Fig. 2.36(a), and let B denote a rectangular array of the same size as A, but in "
"which all values of z are equal to 3 times the mean intensity, z, of the elements of A. Figure 2.36(c) shows "
"the result of performing the set union, in which all values exceeding 3z appear as values from A and all "
"other pixels have value 3z, which is a mid-gray value."
"Before leaving the discussion of sets, we introduce some additional concepts that "
"are  used  later  in  the  book.  The  Cartesian  product  of  two  sets  X  and  Y,  denoted  "
"XY×, is the set of all possible ordered pairs whose first component is a member of "
"X and whose second component is a member of Y. In other words,"
" XYxyx","Xy","Y*=HH(, )","and"
"{}"
" "
"(2-41)"
"For example, if X is a set of M equally spaced values on the x-axis and Y is a set of N "
"equally spaced values on the y-axis, we see that the Cartesian product of these two "
"sets define the coordinates of an M-by-N rectangular array (i.e., the coordinates of "
"an image). As another example, if X and Y denote the specific x- and y-coordinates "
"of a group of 8-connected, 1-valued pixels in a binary image, then set XY× repre-"
"sents the region (object) comprised of those pixels."
"We follow convention "
"in using the symbol × "
"to denote the Cartesian "
"product. This is not to "
"be confused with our "
"use of the same symbol "
"throughout the book "
"to denote the size of "
"an M-by-N image (i.e., "
"M × N)."
"bac"
"FIGURE 2.36"
"Set operations  "
"involving grayscale "
"images. (a) Original  "
"image. (b) Image "
"negative obtained "
"using grayscale set  "
"complementation. "
"(c) The union of "
"image (a) and a "
"constant image. "
"(Original image "
"courtesy of G.E. "
"Medical Systems.)"
"DIP","4E_GLOBAL_Print_Ready.indb   956/16/2017   2:02:41 PM"
"www.EBooks","World.ir"
""
"96"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"A relation  (or,  more  precisely,  a  binary  relation)  on  a  set  A  is  a  collection  of  "
"ordered pairs of elements from A. That is, a binary relation is a subset of the Carte-"
"sian product AA×. A binary relation between two sets, A and B, is a subset of AB×."
"A partial order on a set S is a relation * on S such that * is:"
"(a) reflexive: for any a","SH, aa*;"
"(b) transitive: for any abc  S,,H, ab* and bc* implies that ac*;"
"(c) antisymmetric: for any ab  S,,H ab* and ba* implies that ab=."
"where, for example, ab* reads “a is related to b.” This means that a and b are in set "
"*, which itself is a subset of SS× according to the preceding definition of a relation. "
"A set with a partial order is called a partially ordered set."
"Let the symbol U denote an ordering relation. An expression of the form"
" aaaa"
"n","123"
"UUUU"
"reads: a"
"1"
" precedes a"
"2"
" or is the same as a"
"2"
", a"
"2"
" precedes a"
"3"
" or is the same as a"
"3"
", and so on. "
"When working with numbers, the symbol U typically is replaced by more traditional "
"symbols. For example, the set of real numbers ordered by the relation “less than or "
"equal to” (denoted by ≤"
") is a partially ordered set (see Problem 2.33). Similarly, the "
"set of natural numbers, paired with the relation “divisible by” (denoted by ÷), is a "
"partially ordered set."
"Of more interest to us later in the book are strict orderings. A strict ordering on a "
"set S is a relation * on S, such that * is:"
"(a) antireflexive: for any a","S  aa","H,;¬*"
"(b) transitive: for any abc  S,,,H ab* and bc* implies that ac*."
"where ¬aa*  means  that  a  is  not  related  to  a.  Let  the  symbol  E  denote  a  strict  "
"ordering relation. An expression of the form"
" aaaa"
"n","123"
"EEEE"
"reads a"
"1"
" precedes a"
"2"
", a"
"2"
" precedes a"
"3"
", and so on. A set with a strict ordering is called "
"a strict-ordered set. "
"As an example, consider the set composed of the English alphabet of lowercase "
"letters, Sabc z="
"{}"
",,,  ,. Based on the preceding definition, the ordering"
" abcz","EEE E"
"is strict because no member of the set can precede itself (antireflexivity) and, for any "
"three letters in S, if the first precedes the second, and the second precedes the third, "
"then  the  first  precedes  the  third  (transitivity).  Similarly,  the  set  of  integers  paired  "
"with the relation “less than (<)” is a strict-ordered set. "
"Logical Operations"
"Logical operations deal with TRUE (typically denoted by 1) and FALSE (typically "
"denoted by 0) variables and expressions. For our purposes, this means binary images "
"DIP","4E_GLOBAL_Print_Ready.indb   966/16/2017   2:02:44 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"97"
"composed of foreground (1-valued) pixels, and a background composed of 0-valued "
"pixels. "
"We work with set and logical operators on binary images using one of two basic "
"approaches: (1) we can use the coordinates of individual regions of foreground pix-"
"els in a single image as sets, or (2) we can work with one or more images of the same "
"size and perform logical operations between corresponding pixels in those arrays."
"In the first category, a binary image can be viewed as a Venn diagram in which "
"the  coordinates  of  individual  regions  of  1-valued  pixels  are  treated  as  sets.  The  "
"union of these sets with the set composed of 0-valued pixels comprises the set uni-"
"verse, Æ. In this representation, we work with single images using all the set opera-"
"tions defined in the previous section. For example, given a binary image with two "
"1-valued  regions,  R"
"1"
"  and  R"
"2"
",  we  can  determine  if  the  regions  overlap  (i.e.,  if  they  "
"have  at  least  one  pair  of  coordinates  in  common)  by  performing  the  set  intersec-"
"tion operation RR"
"12"
" ̈ (see Fig. 2.35). In the second approach, we perform logical "
"operations on the pixels of one binary image, or on the corresponding pixels of two "
"or more binary images of the same size. "
"Logical operators can be defined in terms of truth tables, as Table 2.2 shows for "
"two logical variables a and b. The logical AND operation (also denoted ¿) yields a 1 "
"(TRUE)  only  when  both  a and b  are  1.  Otherwise,  it  yields  0  (FALSE).  Similarly,  "
"the  logical  OR  (¡)  yields  1  when  both  a or b  or  both  are  1,  and  0  otherwise.  The  "
"NOT ()  operator  is  self  explanatory.  When  applied  to  two  binary  images,  AND  "
"and OR operate on pairs of corresponding pixels between the images. That is, they "
"are elementwise operators (see the definition of elementwise operators given earlier "
"in this chapter) in this context. The operators AND, OR, and NOT are functionally "
"complete, in the sense that they can be used as the basis for constructing any other "
"logical operator. "
"Figure 2.37 illustrates the logical operations defined in Table 2.2 using the second "
"approach  discussed  above.  The  NOT  of  binary  image  B"
"1"
"  is  an  array  obtained  by  "
"changing all 1-valued pixels to 0, and vice versa. The AND of B"
"1"
" and B"
"2"
" contains a "
"1 at all spatial locations where the corresponding elements of B"
"1"
" and B"
"2"
" are 1; the "
"operation  yields  0’s  elsewhere.  Similarly,  the  OR  of  these  two  images  is  an  array  "
"that  contains  a  1  in  locations  where  the  corresponding  elements  of  B"
"1"
", or  B"
"2"
", or "
"both, are 1. The array contains 0’s elsewhere. The result in the fourth row of Fig. 2.37 "
"corresponds  to  the  set  of  1-valued  pixels  in  B"
"1"
" but  not  in  B"
"2"
". The  last  row  in  the  "
"figure is the XOR (exclusive OR) operation, which yields 1 in the locations where "
"the corresponding elements of B"
"1"
" or B"
"2"
", (but not both) are 1. Note that the logical "
"ab"
"ab","ANDab","ORNOT(a)"
"00001"
"0"
"1011"
"10010"
"11110"
"TABLE 2.2"
"Truth table  "
"defining the "
"logical operators "
"AND(),¿  "
"OR(),¡ and  "
"NO"
"T()."
"DIP","4E_GLOBAL_Print_Ready.indb   976/16/2017   2:02:46 PM"
"www.EBooks","World.ir"
""
"98"
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"expressions in the last two rows of Fig. 2.37 were constructed using operators from "
"Table 2.2; these are examples of the functionally complete nature of these operators."
"We can arrive at the same results in Fig. 2.37 using the first approach discussed "
"above.  To  do  this,  we  begin  by  labeling  the  individual  1-valued  regions  in  each  of  "
"the  two  images  (in  this  case  there  is  only  one  such  region  in  each  image).  Let  A "
"and B denote the set of coordinates of all the 1-valued pixels in images B"
"1"
" and B"
"2"
","
"respectively. Then we form a single array by ORing the two images, while keeping "
"the labels A and B. The result would look like the array BB"
"12"
"OR in Fig. 2.37, but "
"with  the  two  white  regions  labeled  A  and  B.  In  other  words,  the  resulting  array  "
"would look like a Venn diagram. With reference to the Venn diagrams and set opera-"
"tions defined in the previous section, we obtain the results in the rightmost column "
"of  Fig.  2.37  using  set  operations  as  follows:  AB"
"c"
"=NOT(),"
"1"
" AB BB ̈="
"12"
"AND,"
"AB BB ́="
"12"
"OR,  and  similarly  for  the  other  results  in  Fig.  2.37.  We  will  make  "
"extensive use in Chapter 9 of the concepts developed in this section."
"SPATIAL OPERATIONS"
"Spatial  operations  are  performed  directly  on  the  pixels  of  an  image.  We  classify  "
"spatial operations into three broad categories: (1) single-pixel operations, (2) neigh-"
"borhood operations, and (3) geometric spatial transformations."
"FIGURE 2.37"
"Illustration of "
"logical operations "
"involving  "
"foreground "
"(white) pixels. "
"Black represents "
"binary 0’s and "
"white binary 1’s. "
"The dashed lines "
"are shown for  "
"reference only. "
"They are not part "
"of the result. "
"NOT"
"NOT(B"
"1"
")"
"B"
"1"
" AND B"
"2"
"B"
"1"
" OR B"
"2"
"B"
"1"
" AND [NOT (B"
"2"
")]"
"B"
"1"
" XOR B"
"2"
"AND"
"B"
"1"
"B"
"1"
"B"
"2"
"OR"
"XOR"
"AND-"
"NOT"
"DIP","4E_GLOBAL_Print_Ready.indb   986/16/2017   2:02:46 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"99"
"Single-Pixel Operations"
"The simplest operation we perform on a digital image is to alter the intensity of its "
"pixels individually using a transformation function, T, of the form:"
" s","Tz=() "
"(2-42)"
"where z is the intensity of a pixel in the original image and s is the (mapped) inten-"
"sity of the corresponding pixel in the processed image. For example, Fig. 2.38 shows "
"the transformation used to obtain the negative (sometimes called the complement) "
"of  an  8-bit  image.  This  transformation  could  be  used,  for  example,  to  obtain  the  "
"negative image in Fig. 2.36, instead of using sets. "
"Neighborhood Operations"
"Let S"
"xy"
" denote the set of coordinates of a neighborhood (see Section 2.5 regarding "
"neighborhoods) centered on an arbitrary point (, )","xy in an image, f. Neighborhood "
"processing  generates  a  corresponding  pixel  at  the  same  coordinates  in  an  output  "
"(processed) image, g, such that the value of that pixel is determined by a specified "
"operation on the neighborhood of pixels in the input image with coordinates in the "
"set S"
"xy"
". For example, suppose that the specified operation is to compute the average "
"value  of  the  pixels  in  a  rectangular  neighborhood  of  size  mn×  centered  on  (,)","xy"
" "
". "
"The coordinates of pixels in this region are the elements of set S"
"xy"
". Figures 2.39(a) "
"and (b) illustrate the process. We can express this averaging operation as"
" gxy"
"mn"
"frc"
"rc    S"
"xy"
"(, )(, )"
"(, )"
"="
"∑"
"1"
"H"
" "
"(2-43)"
"where r and c are the row and column coordinates of the pixels whose coordinates "
"are in the set S"
"xy"
". Image g is created by varying the coordinates (, )","xy so that the "
"center of the neighborhood moves from pixel to pixel in image f, and then repeat-"
"ing  the  neighborhood  operation  at  each  new  location.  For  instance,  the  image  in  "
"Fig. 2.39(d) was created in this manner using a neighborhood of size 41    41×. The "
"Our use of the word "
"“negative” in this context "
"refers to the digital "
"equivalent of a  "
"photographic negative, "
"not to the numerical "
"negative of the pixels in "
"the image."
"s  T(z)"
"z"
"s"
"0"
"0255z"
"0"
"255"
"FIGURE 2.38"
"Intensity  "
"transformation "
"function used to "
"obtain the digital "
"equivalent of "
"photographic "
"negative of an "
"8-bit image.."
"DIP","4E_GLOBAL_Print_Ready.indb   996/16/2017   2:02:47 PM"
"www.EBooks","World.ir"
""
"100"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"net effect is to perform local blurring in the original image. This type of process is "
"used,  for  example,  to  eliminate  small  details  and  thus  render  “blobs”  correspond-"
"ing to the largest regions of an image. We will discuss neighborhood processing in "
"Chapters 3 and 5, and in several other places in the book. "
"Geometric Transformations"
"We  use  geometric  transformations  modify  the  spatial  arrangement  of  pixels  in  an  "
"image. These transformations are called rubber-sheet transformations because they "
"may be viewed as analogous to “printing” an image on a rubber sheet, then stretch-"
"ing or shrinking the sheet according to a predefined set of rules. Geometric transfor-"
"mations of digital images consist of two basic operations: "
"The value of this pixel"
"is the average value of the"
"pixels in S"
"xy"
"Image f","Image g"
"(x, y)"
"(x, y)"
"S"
"xy"
"m"
"n"
"ba"
"dc"
"FIGURE 2.39"
"Local averaging  "
"using neighbor-"
"hood processing. "
"The procedure is  "
"illustrated in (a) "
"and (b) for a  "
"rectangular  "
"neighborhood.  "
"(c) An aortic  "
"angiogram (see  "
"Section 1.3).  "
"(d) The result of  "
"using Eq. (2-43) "
"with "
"mn==41. "
"T"
"he images are "
"of size 790686× "
"pixels. (Original  "
"image courtesy "
"of Dr. Thomas R. "
"Gest, Division of  "
"Anatomical  "
"Sciences,  "
"University of "
"Michigan Medical "
"School.)"
"DIP","4E_GLOBAL_Print_Ready.indb   1006/16/2017   2:02:47 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"101"
"1. Spatial transformation of coordinates. "
"2. Intensity interpolation that assigns intensity values to the spatially transformed "
"pixels. "
"The transformation of coordinates may be expressed as"
" "
"′"
"′"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"⎡"
"⎣"
"⎢"
"⎤"
"⎦"
"⎥"
"x"
"y"
"x"
"y"
"tt"
"tt"
"x"
"y"
"T"
"1112"
"2122"
" "
"(2-44)"
"where (, )","xy  are  pixel  coordinates  in  the  original  image  and  (, )"
"′′"
"xy  are  the  "
"corresponding  pixel  coordinates  of  the  transformed  image.  For  example,  the  "
"transformation "
"(, ) (   ,   )"
"′′"
"=xyxy","22"
" shrinks the original image to half its size in both "
"spatial directions. "
"Our interest is in so-called affine transformations, which include scaling, translation, "
"rotation, and shearing. The key characteristic of an affine transformation in 2-D is "
"that  it  preserves  points,  straight  lines,  and  planes.  Equation  (2-44)  can  be  used  to  "
"express the transformations just mentioned, except translation, which would require "
"that a constant 2-D vector be added to the right side of the equation. However, it is "
"possible to use homogeneous coordinates to express all four affine transformations "
"using a single 33× matrix in the following general form: "
" "
"′"
"′"
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"x"
"y"
"x"
"y"
"aa"
"a"
"aaa"
"11001"
"111213"
"212223"
"A"
"⎦⎦"
"⎥"
"⎥"
"⎥"
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"x"
"y"
"1"
" "
"(2-45)"
"This transformation can scale, rotate, translate, or sheer an image, depending on the "
"values chosen for the elements of matrix A. Table 2.3 shows the matrix values used "
"to  implement  these  transformations.  A  significant  advantage  of  being  able  to  per-"
"form all transformations using the unified representation in Eq. (2-45) is that it pro-"
"vides the framework for concatenating a sequence of operations. For example, if we "
"want to resize an image, rotate it, and move the result to some location, we simply "
"form  a  33×  matrix  equal  to  the  product  of  the  scaling,  rotation,  and  translation  "
"matrices from Table 2.3 (see Problems 2.36 and 2.37)."
"The preceding transformation moves the coordinates of pixels in an image to new "
"locations. To complete the process, we have to assign intensity values to those loca-"
"tions.  This  task  is  accomplished  using  intensity  interpolation.  We  already  discussed  "
"this topic in Section 2.4. We began that discussion with an example of zooming an "
"image and discussed the issue of intensity assignment to new pixel locations. Zoom-"
"ing is simply scaling, as detailed in the second row of Table 2.3, and an analysis simi-"
"lar to the one we developed for zooming is applicable to the problem of assigning "
"intensity values to the relocated pixels resulting from the other transformations in "
"Table 2.3. As in Section 2.4, we consider nearest neighbor, bilinear, and bicubic inter-"
"polation techniques when working with these transformations."
"We  can  use  Eq.  (2-45)  in  two  basic  ways.  The  first,  is  a  forward  mapping,  which  "
"consists of scanning the pixels of the input image and, at each location (, ),xy com-"
"DIP","4E_GLOBAL_Print_Ready.indb   1016/16/2017   2:02:48 PM"
"www.EBooks","World.ir"
""
"102"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"puting  the  spatial  location  (, )"
"′′"
"xy"
"  of  the  corresponding  pixel  in  the  output  image  "
"using Eq."
" (2-45) directly. A problem with the forward mapping approach is that two "
"or  more  pixels  in  the  input  image  can  be  transformed  to  the  same  location  in  the  "
"output image, raising the question of how to combine multiple output values into a "
"single output pixel value. In addition, it is possible that some output locations may "
"not  be  assigned  a  pixel  at  all.  The  second  approach,  called  inverse  mapping,  scans  "
"the output pixel locations and, at each location (, ),"
"′′"
"xy computes the corresponding "
"location in the input image using (, )( ,  ).xy="
"′′"
"−"
"Axy"
"1"
" It then interpolates (using one "
"of the techniques discussed in Section 2.4) among the nearest input pixels to deter-"
"mine the intensity of the output pixel value. Inverse mappings are more efficient to "
"implement  than  forward  mappings,  and  are  used  in  numerous  commercial  imple-"
"mentations of spatial transformations (for example, MATLAB uses this approach)."
"Transformation"
"Name"
"Affine Matrix, A"
"Coordinate"
"Equations"
"Example"
"Identity"
"1"
"0"
"0"
"0"
"1"
"0"
"0"
"0"
"1"
"x"
"′"
"y"
"′"
"xx="
"′"
"yy="
"′"
"Translation"
"y"
"yyt=+"
"′"
"x"
"xxt=+"
"′"
"10"
"01"
"00 1"
"x"
"y"
"t"
"t"
"⎡⎤"
"⎢⎥"
"⎢⎥"
"⎢⎥"
"⎣⎦"
"Shear (vertical)"
"10"
"010"
"001"
"s"
"⎡⎤"
"⎢⎥"
"⎢⎥"
"⎢⎥"
"⎣⎦"
"v"
"yy="
"′"
"xxsy=+"
"′"
"v"
"Shear (horizontal)"
"100"
"10"
"001"
"h"
"s"
"⎡⎤"
"⎢⎥"
"⎢⎥"
"⎢⎥"
"⎣⎦"
"xx="
"′"
"h"
"ysxy=+"
"′"
"Scaling/Reflection"
"(For reflection, set one "
"scaling factor to −1"
"and the other to 0)"
"c"
"x"
"0"
"0"
"0"
"c"
"y"
"0"
"0"
"0"
"1"
"x"
"xcx="
"′"
"y"
"ycy="
"′"
"Rotation (about the"
"origin)"
"0cos usin u"
"sin ucos u","0"
"001"
"cossinxxy=−"
"′"
"uu"
"sincosyxy=+"
"′"
"uu"
"x"
"′"
"y"
"′"
"x"
"′"
"x"
"′"
"x"
"′"
"x"
"′"
"y"
"′"
"y"
"′"
"y"
"′"
"y"
"′"
"TABLE 2.3"
"Affine  "
"transformations "
"based on  "
"Eq. (2-45)."
"DIP","4E_GLOBAL_Print_Ready.indb   1026/16/2017   2:02:48 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"103"
"EXAMPLE 2.9 :  Image rotation and intensity interpolation."
"The  objective  of  this  example  is  to  illustrate  image  rotation  using  an  affine  transform.  Figure  2.40(a)  "
"shows  a  simple  image  and  Figs.  2.40(b)–(d)  are  the  results  (using  inverse  mapping)  of  rotating  the  "
"original image by −21° (in Table 2.3, clockwise angles of rotation are negative). Intensity assignments "
"were computed using nearest neighbor, bilinear, and bicubic interpolation, respectively. A key issue in "
"image rotation is the preservation of straight-line features. As you can see in the enlarged edge sections "
"in Figs. 2.40(f) through (h), nearest neighbor interpolation produced the most jagged edges and, as in "
"Section 2.4, bilinear interpolation yielded significantly improved results. As before, using bicubic inter-"
"polation produced slightly better results. In fact, if you compare the progression of enlarged detail in "
"Figs. 2.40(f) to (h), you can see that the transition from white (255) to black (0) is smoother in the last "
"figure because the edge region has more values, and the distribution of those values is better balanced. "
"Although the small intensity differences resulting from bilinear and bicubic interpolation are not always "
"noticeable  in  human  visual  analysis,  they  can  be  important  in  processing  image  data,  such  as  in  auto-"
"mated edge following in rotated images."
"The size of the spatial rectangle needed to contain a rotated image is larger than the rectangle of the "
"original image, as Figs. 2.41(a) and (b) illustrate. We have two options for dealing with this: (1) we can "
"crop the rotated image so that its size is equal to the size of the original image, as in Fig. 2.41(c), or we "
"can keep the larger image containing the full rotated original, an Fig. 2.41(d). We used the first option in "
"Fig. 2.40 because the rotation did not cause the object of interest to lie outside the bounds of the original "
"rectangle. The areas in the rotated image that do not contain image data must be filled with some value, 0 "
"(black) being the most common. Note that counterclockwise angles of rotation are considered positive. "
"This is a result of the way in which our image coordinate system is set up (see Fig. 2.19), and the way in "
"which rotation is defined in Table 2.3."
"Image Registration"
"Image  registration  is  an  important  application  of  digital  image  processing  used  to  "
"align  two  or  more  images  of  the  same  scene.  In  image  registration,  we  have  avail-"
"able an input image and a reference image. The objective is to transform the input "
"image geometrically to produce an output image that is aligned (registered) with the "
"reference image. Unlike the discussion in the previous section where transformation "
"functions  are  known,  the  geometric  transformation  needed  to  produce  the  output,  "
"registered image generally is not known, and must be estimated."
"Examples  of  image  registration  include  aligning  two  or  more  images  taken  at  "
"approximately the same time, but using different imaging systems, such as an MRI "
"(magnetic resonance imaging) scanner and a PET (positron emission tomography) "
"scanner. Or, perhaps the images were taken at different times using the same instru-"
"ments, such as satellite images of a given location taken several days, months, or even "
"years apart. In either case, combining the images or performing quantitative analysis "
"and  comparisons  between  them  requires  compensating  for  geometric  distortions  "
"caused by differences in viewing angle, distance, orientation, sensor resolution, shifts "
"in object location, and other factors. "
"DIP","4E_GLOBAL_Print_Ready.indb   1036/16/2017   2:02:49 PM"
"www.EBooks","World.ir"
""
"104"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"One of the principal approaches for solving the problem just discussed is to use tie "
"points (also called control points). These are corresponding points whose locations "
"are known precisely in the input and reference images. Approaches for selecting tie "
"points range from selecting them interactively to using algorithms that detect these "
"points  automatically.  Some  imaging  systems  have  physical  artifacts  (such  as  small  "
"metallic objects) embedded in the imaging sensors. These produce a set of known "
"points (called reseau marks or fiducial marks) directly on all images captured by the "
"system. These known points can then be used as guides for establishing tie points."
"The  problem  of  estimating  the  transformation  function  is  one  of  modeling.  For  "
"example, suppose that we have a set of four tie points each in an input and a refer-"
"ence image. A simple model based on a bilinear approximation is given by"
" xcccc=++    +"
"12  34"
"vwvw "
"(2-46)"
"and"
"ba"
"d"
"c"
"f"
"h"
"e"
"g"
"FIGURE  2.40"
" (a) A 541    421× image of the letter T. (b) Image rotated −21° using nearest-neighbor interpolation for "
"intensity  assignments.  (c)  Image  rotated  −21°  using  bilinear  interpolation.  (d)  Image  rotated  −21°  using  bicubic  "
"interpolation. (e)-(h) Zoomed sections (each square is one pixel, and the numbers shown are intensity values)."
"45"
"154"
"247"
"0"
"0"
"255"
"255255"
"2550"
"00"
"77"
"168"
"255"
"00"
"255"
"DIP","4E_GLOBAL_Print_Ready.indb   1046/16/2017   2:02:49 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"105"
"x"
" ¿"
"y"
" "
"¿"
"y"
" "
"¿"
"x"
" ¿"
"y"
" "
"¿"
"x"
" "
"¿"
"y"
"x"
"Origin  "
"Image f(x, y)"
"Positive"
"angle of"
"rotation"
"Positive"
"angle of"
"rotation"
"ba"
"dc"
"FIGURE 2.41"
"(a) A digital  "
"image.  "
"(b) Rotated image "
"(note the  "
"counterclockwise "
"direction for a "
"positive angle of "
"rotation).  "
"(c) Rotated image "
"cropped to fit the "
"same area as the "
"original image.  "
"(d) Image  "
"enlarged to  "
"accommodate "
"the entire rotated "
"image."
"ycccc=+  +    +"
"56  78"
"vwvw "
"(2-47)"
"During the estimation phase, (,  )","vw and (, )","xy are the coordinates of tie points in the "
"input and reference images, respectively. If we have four pairs of corresponding tie "
"points in both images, we can write eight equations using Eqs. (2-46) and (2-47) and "
"use them to solve for the eight unknown coefficients, c"
"1"
" through c"
"8"
". "
"Once we have the coefficients, Eqs. (2-46) and (2-47) become our vehicle for trans-"
"forming all the pixels in the input image. The result is the desired registered image. "
"After the coefficients have been computed, we let (v,w) denote the coordinates of "
"each pixel in the input image, and (, )","xy become the corresponding coordinates of the "
"output image. The same set of coefficients, c"
"1"
" through c"
"8"
", are used in computing all "
"coordinates (, )","xy; we just step through all (,  )","vw in the input image to generate the "
"corresponding (, )","xy in the output, registered image. If the tie points were selected "
"correctly, this new image should be registered with the reference image, within the "
"accuracy of the bilinear approximation model."
"In  situations  where  four  tie  points  are  insufficient  to  obtain  satisfactory  regis-"
"tration, an approach used frequently is to select a larger number of tie points and "
"then treat the quadrilaterals formed by groups of four tie points as subimages. The "
"subimages  are  processed  as  above,  with  all  the  pixels  within  a  quadrilateral  being  "
"transformed  using  the  coefficients  determined  from  the  tie  points  corresponding  "
"to that quadrilateral. Then we move to another set of four tie points and repeat the "
"DIP","4E_GLOBAL_Print_Ready.indb   1056/16/2017   2:02:50 PM"
"www.EBooks","World.ir"
""
"106"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"procedure until all quadrilateral regions have been processed. It is possible to use "
"more  complex  regions  than  quadrilaterals,  and  to  employ  more  complex  models,  "
"such as polynomials fitted by least squares algorithms. The number of control points "
"and  sophistication  of  the  model  required  to  solve  a  problem  is  dependent  on  the  "
"severity of the geometric distortion. Finally, keep in mind that the transformations "
"defined by Eqs. (2-46) and (2-47), or any other model for that matter, only map the "
"spatial coordinates of the pixels in the input image. We still need to perform inten-"
"sity interpolation using any of the methods discussed previously to assign intensity "
"values to the transformed pixels."
"EXAMPLE 2.10 :  Image registration."
"Figure 2.42(a) shows a reference image and Fig. 2.42(b) shows the same image, but distorted geometri-"
"cally by vertical and horizontal shear. Our objective is to use the reference image to obtain tie points "
"and then use them to register the images. The tie points we selected (manually) are shown as small white "
"squares near the corners of the images (we needed only four tie points because the distortion is linear "
"shear in both directions). Figure 2.42(c) shows the registration result obtained using these tie points in "
"the  procedure  discussed  in  the  preceding  paragraphs.  Observe  that  registration  was  not  perfect,  as  is  "
"evident by the black edges in Fig. 2.42(c). The difference image in Fig. 2.42(d) shows more clearly the "
"slight lack of registration between the reference and corrected images. The reason for the discrepancies "
"is error in the manual selection of the tie points. It is difficult to achieve perfect matches for tie points "
"when distortion is so severe."
"VECTOR AND MATRIX OPERATIONS"
"Multispectral  image  processing  is  a  typical  area  in  which  vector  and  matrix  opera-"
"tions are used routinely. For example, you will learn in Chapter 6 that color images "
"are formed in RGB color space by using red, green, and blue component images, as "
"Fig. 2.43 illustrates. Here we see that each pixel of an RGB image has three compo-"
"nents, which can be organized in the form of a column vector"
" z="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"z"
"z"
"z"
"1"
"2"
"3"
" "
"(2-48)"
"where z"
"1"
" is the intensity of the pixel in the red image, and z"
"2"
" and z"
"3"
" are the corre-"
"sponding pixel intensities in the green and blue images, respectively. Thus, an RGB "
"color  image  of  size  MN×  can  be  represented  by  three  component  images  of  this  "
"size, or by a total of MN vectors of size 31×. A general multispectral case involving "
"n component images (e.g., see Fig. 1.10) will result in n-dimensional vectors:"
" z="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"⎥"
"z"
"z"
"z"
"n"
"1"
"2"
""
" "
"(2-49)"
"Recall that an "
"n-dimensional vector "
"can be thought of as a "
"point in n-dimensional "
"Euclidean space."
"DIP","4E_GLOBAL_Print_Ready.indb   1066/16/2017   2:02:51 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"107"
" We will use this type of vector representation throughout the book."
"The inner product (also called the dot product) of two n-dimensional column vec-"
"tors a and b is defined as"
" "
"ab abi"
""
"T"
"nn"
"ii"
"i"
"n"
"ababa b"
"ab"
"=+ ++"
"="
"="
"∑"
"112 2"
"1"
" "
"(2-50)"
"where T  indicates  the  transpose.  The  Euclidean  vector  norm,  denoted  by  "
"z,  is  "
"defined as the square root of the inner product:"
" "
"zzz="
"("
")"
"T"
"1"
"2"
" "
"(2-51)"
"The product ab"
"T"
" is called "
"the outer product of a "
"and b. It is a matrix of "
"size n × n. "
"ba"
"dc"
"FIGURE 2.42"
"Image  "
"registration. "
"(a) Reference "
"image. (b) Input "
"(geometrically "
"distorted image). "
"Corresponding tie "
"points are shown "
"as small white "
"squares near the "
"corners.  "
"(c) Registered "
"(output) image "
"(note the errors "
"in the border). "
"(d) Difference "
"between (a) and "
"(c), showing more "
"registration errors."
"DIP","4E_GLOBAL_Print_Ready.indb   1076/16/2017   2:02:51 PM"
"www.EBooks","World.ir"
""
"108"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"Component image 3 (Blue)"
"Component image 2 (Green)"
"Component image 1 (Red)"
"z "
"z"
"1"
"z"
"2"
"z"
"3"
"FIGURE 2.43"
"Forming a vector "
"from  "
"corresponding "
"pixel values in "
"three RGB  "
"component  "
"images."
"We recognize this expression as the length of vector z."
"We  can  use  vector  notation  to  express  several  of  the  concepts  discussed  earlier.  "
"For  example,  the  Euclidean  distance,  D(,)","za,  between  points  (vectors)  z  and  a  in  "
"n-dimensional space is defined as the Euclidean vector norm: "
" "
"D"
"zazaz a"
"T"
"nn"
"(, )"
"()( )()"
"za  za    zaza=−=  −"
"()"
"−"
"()"
"⎡"
"⎣"
"⎤"
"⎦"
"=−−−"
"⎡"
"1"
"2"
"11"
"2"
"22"
"22"
"+++"
"⎣⎣"
"⎤"
"⎦"
"1"
"2"
" "
"(2-52)"
"This is a generalization of the 2-D Euclidean distance defined in Eq. (2-19). "
"Another advantage of pixel vectors is in linear transformations, represented as"
" w","Az a=−()"
"(2-53)"
"where A is a matrix of size "
"mn×,"
" and z and a are column vectors of size n×1."
"As noted in Eq. (2-10), entire images can be treated as matrices (or, equivalently, "
"as vectors), a fact that has important implication in the solution of numerous image "
"processing problems. For example, we can express an image of size MN× as a col-"
"umn vector of dimension MN×1 by letting the first M elements of the vector equal "
"the  first  column  of  the  image,  the  next  M  elements  equal  the  second  column,  and  "
"so on. With images formed in this manner, we can express a broad range of linear "
"processes applied to an image by using the notation"
" g","Hf n=+ "
"(2-54)"
"where f is an MN×1 vector representing an input image, n is an MN×1 vector rep-"
"resenting an MN× noise pattern, g is an MN×1 vector representing a processed "
"image,  and  H  is  an  MNMN×  matrix  representing  a  linear  process  applied  to  the  "
"input  image  (see  the  discussion  earlier  in  this  chapter  regarding  linear  processes).  "
"It is possible, for example, to develop an entire body of generalized techniques for "
"image restoration starting with Eq. (2-54), as we discuss in Section 5.9. We will men-"
"tion the use of matrices again in the following section, and show other uses of matri-"
"ces for image processing in numerous chapters in the book."
"DIP","4E_GLOBAL_Print_Ready.indb   1086/16/2017   2:02:52 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"109"
"IMAGE TRANSFORMS"
"All the image processing approaches discussed thus far operate directly on the pixels "
"of  an  input  image;  that  is,  they  work  directly  in  the  spatial  domain.  In  some  cases,  "
"image processing tasks are best formulated by transforming the input images, carry-"
"ing the specified task in a transform domain, and applying the inverse transform to "
"return to the spatial domain. You will encounter a number of different transforms "
"as you proceed through the book. A particularly important class of 2-D linear trans-"
"forms, denoted T(,)","uv, can be expressed in the general form"
" Tfxyrxy"
"y"
"N"
"x"
"M"
"(,)(,)(,,,)","uvuv="
"=="
"∑∑"
"0"
"1"
"0"
"1--"
" "
"(2-55)"
"where fxy(,)  is  an  input  image,  rxy(,,,)","uv  is  called  a  forward  transformation  ker-"
"nel, and Eq. (2-55) is evaluated for u=−0121,, ,  ,...M and v=−0121,, ,  ,...N"
" "
". As "
"before, x and y are spatial variables, while M and N are the row and column dimen-"
"sions of f. Variables u and v are called the transform variables. T(,)","uv is called the "
"forward transform of fxy(,). Given T(,),uv we can recover fxy(,) using the inverse "
"transform of T(,):uv"
" fxy","Tsxy"
"NM"
"(,)(,)(,,,)="
"=="
"∑∑"
"uvuv"
"vu","0"
"1"
"0"
"1--"
" "
"(2-56)"
"for x","M=−0121,, ,  ,... and y","N=−0121,, ,  ,..., where sxy(,,,)","uv is called an inverse "
"transformation kernel. Together, Eqs. (2-55) and (2-56) are called a transform pair."
"Figure 2.44 shows the basic steps for performing image processing in the linear "
"transform domain. First, the input image is transformed, the transform is then modi-"
"fied by a predefined operation and, finally, the output image is obtained by computing "
"the inverse of the modified transform. Thus, we see that the process goes from the "
"spatial domain to the transform domain, and then back to the spatial domain."
"The forward transformation kernel is said to be separable if"
" rxyr y(,,,)(,)","uv)=r(x,uv"
"12"
" "
"(2-57)"
"In  addition,  the  kernel  is  said  to  be  symmetric  if  rx"
"1"
"(, )","u  is  functionally  equal  to  "
"ry"
"2"
"(, )","v"
" "
", so that"
" rxyr xr y(,,,)    (,)(,)","uvuv="
"11"
" "
"(2-58)"
"Identical comments apply to the inverse kernel."
"The nature of a transform is determined by its kernel. A transform of particular "
"importance  in  digital  image  processing  is  the  Fourier  transform,  which  has  the  fol-"
"lowing forward and inverse kernels:"
" rxye"
"jux","M y","N"
"(,,,)"
"()"
"uv"
"v"
"="
"−+2p"
" "
"(2-59)"
"and"
" sx y"
"MN"
"e"
"jux","M y","N"
"(,,,)"
"()"
"uv"
"v"
"="
"+"
"1"
"2p"
" "
"(2-60)"
"DIP","4E_GLOBAL_Print_Ready.indb   1096/16/2017   2:02:54 PM"
"www.EBooks","World.ir"
""
"110"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"respectively, where j=−1 , so these kernels are complex functions. Substituting the "
"preceding kernels into the general transform formulations in Eqs. (2-55) and (2-56) "
"gives us the discrete Fourier transform pair:"
" "
"Tfxye"
"jux","M y","N"
"y"
"N"
"x"
"M"
"(, )(, )"
"()"
"uv"
"v"
"="
"−+"
"=="
"∑∑"
"2"
"0"
"1"
"0"
"1"
"p"
"--"
"(2-61)"
"and"
" "
"fxy"
"MN"
"Te"
"jux","M y","N"
"NM"
"(, )(, )"
"()"
"="
"+"
"=="
"∑∑"
"1"
"2"
"0"
"1"
"0"
"1"
"uv"
"v"
"vu"
"p"
"--"
" "
"(2-62)"
"It can be shown that the Fourier kernels are separable and symmetric (Problem 2.39), "
"and that separable and symmetric kernels allow 2-D transforms to be computed using "
"1-D transforms (see Problem 2.40). The preceding two equations are of fundamental "
"importance in digital image processing, as you will see in Chapters 4 and 5."
"EXAMPLE 2.11 :  Image processing in the transform domain."
"Figure 2.45(a) shows an image corrupted by periodic (sinusoidal) interference. This type of interference "
"can be caused, for example, by a malfunctioning imaging system; we will discuss it in Chapter 5. In the "
"spatial domain, the interference appears as waves of intensity. In the frequency domain, the interference "
"manifests itself as bright bursts of intensity, whose location is determined by the frequency of the sinu-"
"soidal  interference  (we  will  discuss  these  concepts  in  much  more  detail  in  Chapters  4  and  5).  Typi-"
"cally, the bursts are easily observable in an image of the magnitude of the Fourier transform, T(, ).uv "
"With reference to the diagram in Fig. 2.44, the corrupted image is fxy(,), the transform in the leftmost "
"box is the Fourier transform, and Fig. 2.45(b) is T(, )","uv displayed as an image. The bright dots shown "
"are  the  bursts  of  intensity  mentioned  above.  Figure  2.45(c)  shows  a  mask  image  (called  a  filter)  with  "
"white and black representing 1 and 0, respectively. For this example, the operation in the second box of "
"Fig. 2.44 is to multiply the filter by the transform to remove the bursts associated with the interference. "
"Figure 2.45(d) shows the final result, obtained by computing the inverse of the modified transform. The "
"interference is no longer visible, and previously unseen image detail is now made quite clear. Observe, "
"for example, the fiducial marks (faint crosses) that are used for image registration, as discussed earlier."
"When  the  forward  and  inverse  kernels  of  a  transform  are  separable  and  sym-"
"metric, and fxy(,) is a square image of size MM×, Eqs. (2-55) and (2-56) can be "
"expressed in matrix form:"
"The exponential terms "
"in the Fourier transform "
"kernels can be expanded "
"as sines and cosines of "
"various frequencies. As "
"a result, the domain of "
"the Fourier transform "
"is called the frequency "
"domain."
"T(u, v)"
"Transform"
"Operation"
"R"
"Inverse"
"transform"
"Transform domain"
"R[T(u, v)]"
"f(x, y)","g(x, y)"
"Spatial"
"domain"
"Spatial"
"domain"
"FIGURE 2.44"
"General approach "
"for working in the "
"linear transform "
"domain."
"DIP","4E_GLOBAL_Print_Ready.indb   1106/16/2017   2:02:55 PM"
"www.EBooks","World.ir"
""
"2.6"
"  "
"Introduction to the Basic Mathematical Tools Used in Digital Image Processing"
"    "
"111"
" TAFA= "
"(2-63)"
" "
"where F is an MM× matrix containing the elements of fxy(,) [see Eq. (2-9)], A is "
"an MM×  matrix  with  elements  arij"
"ij"
"="
"1"
"(, ),  and  T  is  an  MM×  transform  matrix  "
"with elements T(,),uv for u,v=−0121,, ,  ,....M"
"To  obtain  the  inverse  transform,  we  pre-  and  post-multiply  Eq.  (2-63)  by  an  "
"inverse transformation matrix B:"
" BTBBAFAB= "
"(2-64)"
"If BA="
"−1"
","
" FBTB= "
"(2-65)"
"indicating  that  F  or,  equivalently,  fxy(,),  can  be  recovered  completely  from  its  "
"forward transform. If B is not equal to A"
"−1"
", Eq. (2-65) yields an approximation:"
" "
"ˆ"
"FBAFAB="
" "
"(2-66)"
"In addition to the Fourier transform, a number of important transforms, including "
"the Walsh, Hadamard, discrete cosine, Haar, and slant transforms, can be expressed "
"in the form of Eqs. (2-55) and (2-56), or, equivalently, in the form of Eqs. (2-63) and "
"(2-65). We will discuss these and other types of image transforms in later chapters. "
"ba"
"dc"
"FIGURE 2.45"
"(a) Image  "
"corrupted by  "
"sinusoidal  "
"interference.  "
"(b) Magnitude of "
"the Fourier  "
"transform  "
"showing the "
"bursts of energy "
"caused by the "
"interference "
"(the bursts were "
"enlarged for "
"display purposes). "
"(c) Mask used "
"to eliminate the "
"energy bursts.  "
"(d) Result of  "
"computing the "
"inverse of the "
"modified Fourier "
"transform.  "
"(Original  "
"image courtesy of "
"NASA.) "
"DIP","4E_GLOBAL_Print_Ready.indb   1116/16/2017   2:02:56 PM"
"www.EBooks","World.ir"
""
"112"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"IMAGE INTENSITIES AS RANDOM VARIABLES"
"We treat image intensities as random quantities in numerous places in the book. For "
"example, let zi","L"
"i"
",,,,, ,=−0121... denote the values of all possible intensities in an "
"MN× digital image. The probability, pz"
"k"
"(), of intensity level z"
"k"
" occurring in the im-"
"age is estimated as"
" pz"
"n"
"MN"
"k"
"k"
"()= "
"(2-67)"
"where n"
"k"
" is the number of times that intensity z"
"k"
" occurs in the image and MN is the "
"total number of pixels. Clearly,"
" pz"
"k"
"k"
"L"
"()"
"="
"−"
"∑"
"="
"0"
"1"
"1  "
"(2-68)"
"Once we have pz"
"k"
"(), we can determine a number of important image characteristics. "
"For example, the mean (average) intensity is given by"
" mzpz"
"kk"
"k"
"L"
"="
"="
"−"
"∑"
"()"
"0"
"1"
" "
"(2-69)"
"Similarly, the variance of the intensities is"
" s"
"22"
"0"
"1"
"=−"
"="
"−"
"∑"
"()()","zmpz"
"kk"
"k"
"L"
" "
"(2-70)"
"The variance is a measure of the spread of the values of z about the mean, so it is "
"a useful measure of image contrast. In general, the nth central moment of random "
"variable z about the mean is defined as"
" m"
"nk"
"n"
"k"
"k"
"L"
"zzmpz()()  (  )=−"
"="
"−"
"∑"
"0"
"1"
" "
"(2-71)"
"We  see  that  m"
"0"
"1(),z= m"
"1"
"0(),z=  and  ms"
"2"
"2"
"().z=  Whereas  the  mean  and  variance  "
"have an immediately obvious relationship to visual properties of an image, higher-"
"order  moments  are  more  subtle.  For  example,  a  positive  third  moment  indicates  "
"that the intensities are biased to values higher than the mean, a negative third mo-"
"ment would indicate the opposite condition, and a zero third moment would tell us "
"that the intensities are distributed approximately equally on both sides of the mean. "
"These features are useful for computational purposes, but they do not tell us much "
"about the appearance of an image in general."
"As you will see in subsequent chapters, concepts from probability play a central "
"role  in  a  broad  range  of  image  processing  applications.  For  example,  Eq.  (2-67)  is  "
"utilized in Chapter 3 as the basis for image enhancement techniques based on his-"
"tograms. In Chapter 5, we use probability to develop image restoration algorithms, "
"in Chapter 10 we use probability for image segmentation, in Chapter 11 we use it "
"to describe texture, and in Chapter 12 we use probability as the basis for deriving "
"optimum pattern recognition algorithms."
"You may find it useful "
"to  consult the tutorials "
"section in the book "
"website for a brief review "
"of probability."
"DIP","4E_GLOBAL_Print_Ready.indb   1126/16/2017   2:02:58 PM"
"www.EBooks","World.ir"
""
" "
"  "
"Summary, References, and Further Reading"
"    "
"113"
"Problems"
" "
"Solutions to the problems marked with an asterisk (*) are in the DIP","4E Student Support Package (consult the book "
"website: www.Image","Processing","Place.com)."
"2.1 If  you  use  a  sheet  of  white  paper  to  shield  your  "
"eyes when looking directly at the sun, the side of "
"the sheet facing you appears black. Which of the "
"visual processes discussed in Section 2.1 is respon-"
"sible for this?"
"2.2 * Using  the  background  information  provided  in  "
"Section  2.1,  and  thinking  purely  in  geometrical  "
"terms,   estimate   the   diameter   of   the   smallest   "
"printed  dot  that  the  eye  can  discern  if  the  page  "
"on which the dot is printed is 0.2 m away from the "
"eyes. Assume for simplicity that the visual system "
"ceases to detect the dot when the image of the dot "
"on  the  fovea  becomes  smaller  than  the  diameter  "
"of  one  receptor  (cone)  in  that  area  of  the  retina.  "
"Assume further that the fovea can be modeled as "
"a  square  array  of  dimension  "
"15.mm  on  the  side,  "
"and that the cones and spaces between the cones "
"are distributed uniformly throughout this array."
"2.3 Although  it  is  not  shown  in  Fig.  2.10,  alternating  "
"current  is  part  of  the  electromagnetic  spectrum.  "
"Commercial  alternating  current  in  the  United  "
"States has a frequency of 60 Hz. What is the wave-"
"length  in  kilometers  of  this  component  of  the  "
"spectrum?"
"2.4 You are hired to design the front end of an imag-"
"ing system for studying the shapes of cells, bacteria, "
"viruses,  and  proteins.  The  front  end  consists  in  "
"this  case  of  the  illumination  source(s)  and  cor-"
"responding  imaging  camera(s).The  diameters  of  "
"circles  required  to  fully  enclose  individual  speci-"
"mens in each of these categories are 50, 1, 0.1, and "
"001.mm,  respectively.  In  order  to  perform  auto-"
"mated analysis, the smallest detail discernible on a "
"specimen must be 0 001.mm. "
"(a) * Can  you  solve  the  imaging  aspects  of  this  "
"problem  with  a  single  sensor  and  camera?  "
"If  your  answer  is  yes,  specify  the  illumina-"
"tion wavelength band and the type of camera "
"needed. By “type,” we mean the band of the "
"electromagnetic  spectrum  to  which  the  cam-"
"era is most sensitive (e.g., infrared)."
"(b) If  your  answer  in  (a)  is  no,  what  type  of  illu-"
"mination sources and corresponding imaging "
"sensors  would  you  recommend?  Specify  the  "
"light  sources  and  cameras  as  requested  in  "
"part (a). Use the minimum number of illumi-"
"nation  sources  and  cameras  needed  to  solve  "
"the  problem.  (Hint:  From  the  discussion  in  "
"Summary, References, and Further Reading "
"The material in this chapter is the foundation for the remainder of the book. For additional reading on visual per-"
"ception, see Snowden et al. [2012], and the classic book by Cornsweet [1970]. Born and Wolf [1999] discuss light in "
"terms of electromagnetic theory. A basic source for further reading on image sensing is Trussell and Vrhel [2008]. "
"The image formation model discussed in Section 2.3 is from Oppenheim et al. [1968]. The IES Lighting Handbook "
"[2011] is a reference for the illumination and reflectance values used in that section. The concepts of image sampling "
"introduced  in  Section  2.4  will  be  covered  in  detail  in  Chapter  4.  The  discussion  on  experiments  dealing  with  the  "
"relationship between image quality and sampling is based on results from Huang [1965]. For further reading on the "
"topics discussed in Section 2.5, see Rosenfeld and Kak [1982], and Klette and Rosenfeld [2004]."
"See Castleman [1996] for additional reading on linear systems in the context of image processing. The method of "
"noise reduction by image averaging was first proposed by Kohler and Howell [1963]. See Ross [2014] regarding the "
"expected value of the mean and variance of the sum of random variables. See Schröder [2010] for additional read-"
"ing on logic and sets. For additional reading on geometric spatial transformations see Wolberg [1990] and Hughes "
"and Andries [2013]. For further reading on image registration see Goshtasby [2012]. Bronson and Costa [2009] is a "
"good reference for additional reading on vectors and matrices. See Chapter 4 for a detailed treatment of the Fourier "
"transform, and Chapters 7, 8, and 11 for details on other image transforms. For details on the software aspects of "
"many of the examples in this chapter, see Gonzalez, Woods, and Eddins [2009]. "
"DIP","4E_GLOBAL_Print_Ready.indb   1136/16/2017   2:02:58 PM"
"www.EBooks","World.ir"
""
"114"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"Section 2.2, the illumination required to “see” "
"an  object  must  have  a  wavelength  the  same  "
"size or smaller than the object.)"
"2.5 You are preparing a report and have to insert in it "
"an image of size "
"20482048× pixels."
"(a) * Assuming no limitations on the printer, what "
"would  the  resolution  in  line  pairs  per  mm  "
"have  to  be  for  the  image  to  fit  in  a  space  of  "
"size "
"55× cm?"
"(b) What would the resolution have to be in dpi "
"for the image to fit in 22× inches?"
"2.6 * A CCD camera chip of dimensions 77× mm and "
"1024    1024×  sensing  elements,  is  focused  on  a  "
"square, flat area, located 0.5 m away. The camera "
"is  equipped  with  a  35-mm  lens.  How  many  line  "
"pairs per mm will this camera be able to resolve? "
"(Hint:  Model  the  imaging  process  as  in  Fig.  2.3,  "
"with the focal length of the camera lens substitut-"
"ing for the focal length of the eye.)"
"2.7 An  automobile  manufacturer  is  automating  the  "
"placement of certain components on the bumpers "
"of  a  limited-edition  line  of  sports  cars.  The  com-"
"ponents  are  color-coordinated,  so  the  assembly    "
"robots need to know the color of each car in order "
"to   select   the   appropriate   bumper   component.   "
"Models come in only four colors: blue, green, red, "
"and  white.  You  are  hired  to  propose  a  solution  "
"based on imaging. How would you solve the prob-"
"lem of determining the color of each car, keeping "
"in  mind  that  cost  is  the  most  important  consider-"
"ation in your choice of components?"
"2.8 * Suppose that a given automated imaging applica-"
"tion requires a minimum resolution of 5 line pairs "
"per  mm  to  be  able  to  detect  features  of  interest  "
"in  objects  viewed  by  the  camera.  The  distance  "
"between the focal center of the camera lens and "
"the  area  to  be  imaged  is  1  m.  The  area  being  "
"imaged  is  "
"055.×0"
".  m.  You  have  available  a  200  "
"mm  lens,  and  your  job  is  to  pick  an  appropriate  "
"CCD imaging chip. What is the minimum number "
"of sensing elements and square size, "
"dd×,"
" of the "
"CCD chip that will meet the requirements of this "
"application?  (Hint:  Model  the  imaging  process  "
"as  in  Fig.  2.3,  and  assume  for  simplicity  that  the  "
"imaged area is square.)"
"2.9 A  common  measure  of  transmission  for  digital  "
"data  is  the  baud  rate,  defined  as  symbols  (bits  in  "
"our case) per second. As a minimum, transmission "
"is  accomplished  in  packets  consisting  of  a  start  "
"bit,  a  byte  (8  bits)  of  information,  and  a  stop  bit.  "
"Using these facts, answer the following:"
"(a) * How many seconds would it take to transmit "
"a sequence of 500 images of size "
"1024    1024× "
"pixels  with  256  intensity  levels  using  a  3  "
"M-baud (10"
"6"
"bits/sec)  baud  modem?  (This  "
"is a representative medium speed for a DSL "
"(Digital Subscriber Line) residential line."
"(b) What  would  the  time  be  using  a  30  G-baud  "
"(10"
"9"
"bits/sec)"
"  modem?  (This  is  a  represen-"
"tative medium speed for a commercial line.)"
"2.10 * High-definition   television   (HDTV)   generates   "
"images  with  1125  horizontal  TV  lines  interlaced  "
"(i.e.,  where  every  other  line  is  “painted”  on  the  "
"screen  in  each  of  two  fields,  each  field  being  "
"160th  of  a  second  in  duration).  The  width-to-"
"height  aspect  ratio  of  the  images  is  16:9.  The  "
"fact  that  the  number  of  horizontal  lines  is  fixed  "
"determines  the  vertical  resolution  of  the  images.  "
"A  company  has  designed  a  system  that  extracts  "
"digital images from HDTV video. The resolution "
"of  each  horizontal  line  in  their  system  is  propor-"
"tional  to  vertical  resolution  of  HDTV,  with  the  "
"proportion being the width-to-height ratio of the "
"images. Each pixel in the color image has 24 bits "
"of  intensity,  8  bits  each  for  a  red,  a  green,  and  a  "
"blue  component  image.  These  three  “primary”  "
"images form a color image. How many bits would "
"it  take  to  store  the  images  extracted  from  a  two-"
"hour HDTV movie?"
"2.11 When  discussing  linear  indexing  in  Section  2.4,  "
"we  arrived  at  the  linear  index  in  Eq.  (2-14)  by  "
"inspection. The same argument used there can be "
"extended to a 3-D array with coordinates x, y, and "
"z, and corresponding dimensions M, N, and P. The "
"linear index for any "
"(,,)","xyz is"
"sx","My","Nz=++()"
"Start with this expression and"
"(a) * Derive Eq. (2-15)."
"(b) Derive Eq. (2-16)."
"2.12 * Suppose that a flat area with center at "
"(, )","xy"
"00"
" is "
"DIP","4E_GLOBAL_Print_Ready.indb   1146/16/2017   2:02:59 PM"
"www.EBooks","World.ir"
""
" "
"  "
"Problems"
"    "
"115"
"illuminated by a light source with intensity distri-"
"bution"
" ixy","Ke"
"xxyy"
"(, )"
"[()()  ]"
"="
"−−   +−"
"0"
"2"
"0"
"2"
"Assume   for   simplicity   that   the   reflectance   of   "
"the  area  is  constant  and  equal  to  1.0,  and  let  "
"K=255."
" If the intensity of the resulting image is "
"quantized using k bits, and the eye can detect an "
"abrupt  change  of  eight  intensity  levels  between  "
"adjacent pixels, what is the highest value of k that "
"will cause visible false contouring?"
"2.13 Sketch the image in Problem 2.12 for "
"k=2."
"2.14 Consider the two image subsets, S"
"1"
" and S"
"2"
" in the "
"following  figure.  With  reference  to  Section  2.5,  "
"and  assuming  that  "
"V="
"{}"
"1,"
"  determine  whether  "
"these two subsets are:"
"(a) *  4-adjacent."
"(b)  8-adjacent. "
"(c) m-adjacent. "
"1"
"S"
"2"
"S"
"0000000011"
"1110010000"
"1011010000"
"0000111000"
"0100111011"
"2.15 * Develop an algorithm for converting a one-pixel-"
"thick 8-path to a 4-path."
"2.16 Develop an algorithm for converting a one-pixel-"
"thick m-path to a 4-path."
"2.17 Refer  to  the  discussion  toward  the  end  of  Sec-"
"tion 2.5, where we defined the background of an "
"image  as  "
"(),R"
"u"
"c"
"  the  complement  of  the  union  of  "
"all the regions in the image. In some applications, "
"it is advantageous to define the background as the "
"subset  of  pixels  of  "
"()","R"
"u"
"c"
"  that  are  not  hole  pixels  "
"(informally,  think  of  holes  as  sets  of  background  "
"pixels  surrounded  by  foreground  pixels).  How  "
"would you modify the definition to exclude hole "
"pixels  from  "
"()","R"
"u"
"c"
"?  An  answer  such  as  “the  back-"
"ground is the subset of pixels of ()","R"
"u"
"c"
" that are not "
"hole pixels” is not acceptable. (Hint: Use the con-"
"cept of connectivity.)"
"2.18 Consider the image segment shown in the figure "
"that follows."
"(a) * As  in  Section  2.5,  let  V={,}01  be  the  set  "
"of  intensity  values  used  to  define  adjacency.  "
"Compute  the  lengths  of  the  shortest  4-,  8-,  "
"and m-path  between  p and q  in  the  follow-"
"ing image. If a particular path does not exist "
"between these two points, explain why."
"3121"
"2202"
"1211"
"1012(p)"
"(q)"
"(b) Repeat (a) but using V={,}.12"
"2.19 Consider two points p and q."
"(a) * State  the  condition(s)  under  which  the  D"
"4"
" "
"distance  between  p  and  q  is  equal  to  the  "
"shortest 4-path between these points."
"(b) Is this path unique?"
"2.20 Repeat problem 2.19 for the D"
"8"
" distance."
"2.21 Consider  two  one-dimensional  images  f  and  g  of  "
"the same size. What has to be true about the ori-"
"entation of these images for the elementwise and "
"matrix products discussed in Section 2.6 to make "
"sense?  Either  of  the  two  images  can  be  first  in  "
"forming the product."
"2.22 * In  the  next  chapter,  we  will  deal  with  operators  "
"whose function is to compute the sum of pixel val-"
"ues in a small subimage area, "
"S"
"xy"
", as in Eq. (2-43). "
"Show that these are linear operators."
"2.23 Refer to Eq. (2-24) in answering the following: "
"(a) * Show that image summation is a linear opera-"
"tion."
"(b) Show that image subtraction is a linear oper-"
"ation."
"(c) * Show that image multiplication in a nonlinear "
"operation."
"(d) Show that image division is a nonlinear opera-"
"tion."
"2.24 The  median,  z,   of  a  set  of  numbers  is  such  that  "
"DIP","4E_GLOBAL_Print_Ready.indb   1156/16/2017   2:03:00 PM"
"www.EBooks","World.ir"
""
"116"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"half the values in the set are below z and the oth-"
"er  half  are  above  it.  For  example,  the  median  of  "
"the  set  of  values  {,,,  ,  ,  ,  }23"
"820212531  is  20.  Show  "
"that  an  operator  that  computes  the  median  of  a  "
"subimage  area,  S,  is  nonlinear.  (Hint:  It  is  suffi-"
"cient  to  show  that  "
"z  fails  the  linearity  test  for  a  "
"simple numerical example"
".)"
"2.25 * Show  that  image  averaging  can  be  done  recur-"
"sively.  That  is,  show  that  if  ak()","is  the  average  of  "
"k  images"
",  then  the  average  of  k+1  images  can  "
"be obtained from the already-computed average, "
"ak(), and the new image, f"
"k+1"
". "
"2.26With reference to Example 2.5:"
"(a) * Prove the validity of Eq. (2-27)."
"(b) Prove the validity of Eq. (2-28)."
"For part (b) you will need the following facts from "
"probability: (1) the variance of a constant times a "
"random variable is equal to the constant squared "
"times the variance of the random variable. (2) The "
"variance of the sum of uncorrelated random vari-"
"ables  is  equal  to  the  sum  of  the  variances  of  the  "
"individual random variables."
"2.27 Consider two 8-bit images whose intensity levels "
"span the full range from 0 to 255."
"(a) * Discuss the limiting effect of repeatedly sub-"
"tracting  image  (2)  from  image  (1).  Assume  "
"that  the  results  have  to  be  represented  also  "
"in eight bits."
"(b) Would  reversing  the  order  of  the  images  "
"yield a different result?"
"2.28 * Image subtraction is used often in industrial appli-"
"cations for detecting missing components in prod-"
"uct assembly. The approach is to store a “golden” "
"image that corresponds to a correct assembly; this "
"image is then subtracted from incoming images of "
"the  same  product.  Ideally,  the  differences  would  "
"be  zero  if  the  new  products  are  assembled  cor-"
"rectly.  Difference  images  for  products  with  miss-"
"ing  components  would  be  nonzero  in  the  area  "
"where  they  differ  from  the  golden  image.  What  "
"conditions  do  you  think  have  to  be  met  in  prac-"
"tice for this method to work?"
"2.29 With reference to Eq. (2-32),"
"(a) * Give  a  general  formula  for  the  value  of  K "
"as  a  function  of  the  number  of  bits,  k,  in  an  "
"image, such that K results in a scaled image "
"whose intensities span the full k-bit range."
"(b) Find K for 16- and 32-bit images. "
"2.30 Give  Venn  diagrams  for  the  following  expres-"
"sions:"
"(a) * "
"()().AC    ABC ̈ ̈ ̈−"
"(b) "
"()().AC   BC ̈ ́ ̈"
"(c) BABABC−−"
"[]"
"()() ̈ ̈ ̈"
"(d) BB ACAC−=∅ ̈ ́ ̈();.Given that "
"2.31 Use  Venn  diagrams  to  prove  the  validity  of  the  "
"following expressions:"
"(a) * "
"()()()","ABAC  ABC  A BC ̈ ́   ̈ ̈ ̈ ̈ ́−"
"[]"
"="
"(b) ()","ABCA  B  C"
"c  ccc"
" ́ ́ ̈   ̈="
"(c) ()   ()","AC    B   B A  C"
"c"
" ́ ̈=−−"
"(d) ()","ABCA  B  C"
"c  ccc"
" ̈ ̈ ́   ́="
"2.32 Give  expressions  (in  terms  of  sets  A, B,  and  C) "
"for the sets shown shaded in the following figures. "
"The shaded areas in each figure constitute one set, "
"so  give  only  one  expression  for  each  of  the  four  "
"figures."
"(a)*(b)(c)(d)"
"A"
"B"
"C"
"2.33 With reference to the discussion on sets in Section "
"2.6, do the following:"
"(a) * Let S be a set of real numbers ordered by the "
"relation  “less  than  or  equal  to”  "
"().≤"
"  Show  "
"that S is a partially ordered set; that is, show "
"that the reflexive, transitive, and antisymmet-"
"ric properties hold."
"(b) * Show that changing the relation “less than or "
"equal to” to “less than” "
"()<"
" produces a strict "
"ordered set."
"(c) Now let S be the set of lower-case letters in "
"the English alphabet. Show that, under "
"(),<"
"S is a strict ordered set."
"2.34 For any nonzero integers m and n, we say that m "
"DIP","4E_GLOBAL_Print_Ready.indb   1166/16/2017   2:03:01 PM"
"www.EBooks","World.ir"
""
" "
"  "
"Problems"
"    "
"117"
"is  divisible  by  n,  written  "
"mn"
",  if  there  exists  an  "
"integer k such that "
"knm=."
" For example, 42 (m) "
"is  divisible  by  7  (n)  because  there  exists  an  inte-"
"ger "
"k=6"
" such that "
"knm=."
" Show that the set of "
"positive  integers  is  a  partially  ordered  set  under  "
"the relation “divisible by.” In other words, do the "
"following:"
"(a) * Show  that  the  property  of  reflectivity  holds  "
"under this relation."
"(b) Show that the property of transitivity holds."
"(c) Show that anti symmetry holds."
"2.35 In general, what would the resulting image, "
"gxy(,), "
"look like if we modified Eq. (2-43), as follows:"
"gxy"
"mn"
"Tfrc"
"rc   S"
"xy"
"(, )(,)"
"(, )"
"="
"[]"
"∑"
"1"
"H"
"where T  is  the  intensity  transformation  function  "
"in Fig. 2.38(b)?"
"2.36 With  reference  to  Table  2.3,  provide  single,  com-"
"posite  transformation  functions  for  performing  "
"the following operations:"
"(a) * Scaling and translation."
"(b) * Scaling, translation, and rotation."
"(c) Vertical  shear,  scaling,  translation,  and  rota-"
"tion."
"(d) Does the order of multiplication of the indi-"
"vidual  matrices  to  produce  a  single  transfor-"
"mations make a difference? Give an example "
"based  on  a  scaling/translation  transforma-"
"tion to support your answer."
"2.37 We  know  from  Eq.  (2-45)  that  an  affine  transfor-"
"mation of coordinates is given by "
"′"
"′"
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"="
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"x"
"y"
"x"
"y"
"aaa"
"aaa"
"11001"
"111213"
"212223"
"A"
"⎦⎦"
"⎥"
"⎥"
"⎥"
"⎡"
"⎣"
"⎢"
"⎢"
"⎢"
"⎤"
"⎦"
"⎥"
"⎥"
"⎥"
"x"
"y"
"1"
"where "
"(, )"
"′′"
"xy  are  the  transformed  coordinates,  "
"(, )","xy  are  the  original  coordinates,  and  the  ele-"
"ments  of  A  are  given  in  Table  2.3  for  various  "
"types of transformations. The inverse transforma-"
"tion, "
"A"
"−1"
",  to go from the transformed back to the "
"original  coordinates  is  just  as  important  for  per-"
"forming inverse mappings."
"(a) * Find the inverse scaling transformation."
"(b) Find the inverse translation transformation."
"(c) Find   the   inverse   vertical   and   horizontal   "
"shearing transformations."
"(d) * Find the inverse rotation transformation."
"(e) * Show  a  composite  inverse  translation/rota-"
"tion transformation."
"2.38 What are the equations, analogous to Eqs. (2-46) "
"and  (2-47),  that  would  result  from  using  triangu-"
"lar instead of quadrilateral regions?"
"2.39 Do the following."
"(a) * Prove that the Fourier kernel in Eq. (2-59) is "
"separable and symmetric."
"(b) Repeat (a) for the kernel in Eq. (2-60)."
"2.40 * Show  that  2-D  transforms  with  separable,  sym-"
"metric  kernels  can  be  computed  by:  (1)  comput-"
"ing 1-D transforms along the individual rows (col-"
"umns) of the input image; and (2) computing 1-D "
"transforms along the columns (rows) of the result "
"from step (1)."
"2.41 A plant produces miniature polymer squares that "
"have  to  undergo  100%  visual  inspection.  Inspec-"
"tion  is  semi-automated.  At  each  inspection  sta-"
"tion, a robot places each polymer square over an "
"optical  system  that  produces  a  magnified  image  "
"of  the  square.  The  image  completely  fills  a  view-"
"ing screen of size "
"8080× mm. Defects appear as "
"dark circular blobs, and the human inspector’s job "
"is to look at the screen and reject any sample that "
"has one or more dark blobs with a diameter of 0.8 "
"mm  or  greater,  as  measured  on  the  scale  of  the  "
"screen. The manufacturing manager believes that "
"if she can find a way to fully automate the process, "
"profits  will  increase  by  50%,  and  success  in  this  "
"project will aid her climb up the corporate ladder. "
"After extensive investigation, the manager decides "
"that the way to solve the problem is to view each "
"inspection screen with a CCD TV camera and feed "
"the output of the camera into an image processing "
"system  capable  of  detecting  the  blobs,  measuring  "
"their  diameter,  and  activating  the  accept/reject  "
"button  previously  operated  by  a  human  inspec-"
"tor. She is able to find a suitable system, provided "
"that  the  smallest  defect  occupies  an  area  of  at  "
"least "
"22× pixels in the digital image. The manager "
"hires you to help her specify the camera and lens "
"DIP","4E_GLOBAL_Print_Ready.indb   1176/16/2017   2:03:02 PM"
"www.EBooks","World.ir"
""
"118"
"    "
"Chapter"
" "
"2"
"  "
"Digital Image Fundamentals"
"system  to  satisfy  this  requirement,  using  off-the-"
"shelf  components.  Available  off-the-shelf  lenses  "
"have  focal  lengths  that  are  integer  multiples  of  "
"25  mm  or  35  mm,  up  to  200  mm.  Available  cam-"
"eras  yield  image  sizes  of  "
"512512×,   1024    1024×, "
"or "
"20482048×"
"  pixels.  The  individual  imaging  "
"elements in these cameras are squares measuring "
"88×    m,m  and  the  spaces  between  imaging  ele-"
"ments are 2   m.m","For this application, the cameras "
"cost much more than the lenses"
", so you should use "
"the  lowest-resolution  camera  possible,  consistent  "
"with  a  suitable  lens.  As  a  consultant,  you  have  "
"to  provide  a  written  recommendation,  showing  "
"in  reasonable  detail  the  analysis  that  led  to  your  "
"choice of components. Use the imaging geometry "
"suggested in Problem 2.6."
"DIP","4E_GLOBAL_Print_Ready.indb   1186/16/2017   2:03:03 PM"
"www.EBooks","World.ir"